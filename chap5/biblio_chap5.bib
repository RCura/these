
@article{schelling_dynamic_1971,
  title = {Dynamic Models of Segregation},
  volume = {1},
  number = {2},
  journaltitle = {Journal of mathematical sociology},
  date = {1971},
  pages = {143--186},
  author = {Schelling, Thomas C},
  note = {04241}
}

@article{roth_interactive_2013,
  title = {Interactive Maps: {{What}} We Know and What We Need to Know},
  issn = {1948-660X},
  url = {http://www.josis.org/index.php/josis/article/view/105},
  doi = {10/gfw9mq},
  shorttitle = {Interactive Maps},
  number = {6},
  journaltitle = {Journal of Spatial Information Science},
  date = {2013-06-15},
  author = {Roth, Robert E.},
  file = {/home/robin/Dropbox/Biblio/Roth_2013_Interactive_maps.pdf},
  note = {00099}
}

@thesis{amirpour_amraii_human-data_2018,
  title = {Human-{{Data Interaction}} in Large and {{High}}-{{Dimensional Data}}},
  institution = {{University of Pittsburgh}},
  type = {PhD Thesis},
  date = {2018},
  author = {Amirpour Amraii, Saman},
  file = {/home/robin/Dropbox/Biblio/Amirpour Amraii/Amirpour_Amraii_2018_Human-Data_Interaction_in_large_and.pdf},
  note = {00000}
}

@article{liu_effects_2014,
  title = {The {{Effects}} of {{Interactive Latency}} on {{Exploratory Visual Analysis}}},
  volume = {20},
  issn = {1077-2626},
  doi = {10/f3tvrw},
  abstract = {To support effective exploration, it is often stated that interactive visualizations should provide rapid response times. However, the effects of interactive latency on the process and outcomes of exploratory visual analysis have not been systematically studied. We present an experiment measuring user behavior and knowledge discovery with interactive visualizations under varying latency conditions. We observe that an additional delay of 500ms incurs significant costs, decreasing user activity and data set coverage. Analyzing verbal data from think-aloud protocols, we find that increased latency reduces the rate at which users make observations, draw generalizations and generate hypotheses. Moreover, we note interaction effects in which initial exposure to higher latencies leads to subsequently reduced performance in a low-latency setting. Overall, increased latency causes users to shift exploration strategy, in turn affecting performance. We discuss how these results can inform the design of interactive analysis tools.},
  number = {12},
  journaltitle = {IEEE Transactions on Visualization and Computer Graphics},
  date = {2014-12},
  pages = {2122-2131},
  keywords = {Data visualization,Image color analysis,Humans,Computer Graphics,data analysis,Cognition,data mining,Databases; Factual,delay,exploratory analysis,exploratory visual analysis,Informatics,Interaction,interactive analysis tools,interactive latency effects,Interactive services,interactive systems,interactive visualization,knowledge discovery,latency,scalability,Task Performance and Analysis,think-aloud protocols,Time Factors,user behavior,user performance,verbal analysis,verbal data analysis,Visual Analysis,Visualisation},
  author = {Liu, Z. and Heer, J.},
  file = {/home/robin/Dropbox/Biblio/Liu_Heer/Liu_Heer_2014_The_Effects_of_Interactive_Latency_on_Exploratory.pdf},
  note = {00125}
}

@misc{pafka_benchm-databases_2017,
  langid = {english},
  title = {Benchm-Databases: {{A}} Minimal Benchmark of Various Tools (Statistical Software, Databases Etc.) for Working with Tabular Data of Moderately Large Sizes (Interactive Data Analysis)},
  url = {https://github.com/szilard/benchm-databases},
  shorttitle = {Benchm-Databases},
  urldate = {2018-08-25},
  date = {2017-07-25},
  author = {Pafka, Szilard},
  origdate = {2015-02-25T04:56:02Z},
  note = {00000}
}

@article{hegselmann_thomas_2017,
  title = {Thomas {{C}}. {{Schelling}} and {{James M}}. {{Sakoda}}: {{The Intellectual}}, {{Technical}}, and {{Social History}} of a {{Model}}},
  volume = {20},
  issn = {1460-7425},
  doi = {10/gdm894},
  shorttitle = {Thomas {{C}}. {{Schelling}} and {{James M}}. {{Sakoda}}},
  number = {3},
  journaltitle = {Journal of Artificial Societies and Social Simulation},
  shortjournal = {JASSS},
  date = {2017},
  pages = {15},
  author = {Hegselmann, Rainer},
  note = {00000}
}

@unpublished{cura_making_2017,
  venue = {{York, United-Kingdom}},
  title = {Making Large Spatio-Temporal Data Analysis Easier. {{Illustrated}} Plea for Using (Geo){{Visual Analytics}}.},
  url = {http://www.geog.leeds.ac.uk/ectqg17/home.html},
  abstract = {In 1998, Peter Fisher advocated for an improvement of geographic data exploration, considering that “software tools need to be developed to transform data into intelligible views and exploratory tools. [...] It must be made much easier” (Fisher, 1998). For Andrienko et al. (2014), this issue “remains sharply pertinent nearly 20 years later”.
Fact is, despite being able to rely on some very useful and praised tools like GeoDa, most of the geographical data is still explored and analyzed within the bounds of traditional GIS software, especially within the Theoretical and Quantitative Geography community.
Alan MacEachren (2017) called for a renewed use of (Geo)Visual Analytics. He especially insists on the opportunities that Big Data offers, pleading “for a ‘human-in-the-loop’ (geo)Visual Analytics approach to leveraging big (geo)data”.
I suggest that this approach can also be applied to regular spatio-temporal large datasets. Together with the emergence of the “geodata science” and the growing complexity of analysis methods, most of the recent research on this field focus on developing new complex methods. With today’s computation power increase and more employable technologies, TQG community could make use of the simplest visual exploration methods that could not be applied on large dataset until recently. Among these, I consider traditional methods such as small multiples, linked views and filters, temporal as well as spatial and thematic.
In this talk, I’ll present such a use on three large and inherently spatial and temporal datasets. (1) The first comes from the Google Location History of an anonymous user. It consists of a frequent recording (every 3 minutes) of the user’s smartphone location. Albeit the lack of contextual data, we will show how this mass of data can allow to gather very personal informations about the user. (2) We also present an exploration of the CitiBike dataset. Originating from New-York City bike sharing system, it consists of a record for each trip. Together with spatio-temporal informations, thematic data like age and gender of users can help figure out different uses of the bike system through time and space. (3) The last dataset is about transit : for multiple road segments, road-censors in Paris registers the car traffic every hour. Gathered over a year, this very large dataset can help detecting patterns and special events in the car flow.
Those datasets and questions were addressed by creating specific web applications dedicated to their visual exploration, and I will try to expose the generality of such an approach. A particular attention will be dedicated to showing how easy it can be to build such ad-hoc exploratory tools, and, more importantly, how simple it can be to use.},
  eventtitle = {20th {{European Colloquium}} on {{Theoretical}} and {{Quantitative Geography}} ({{ECTQG}} 2017)},
  date = {2017-09-11},
  author = {Cura, Robin},
  file = {/home/robin/Zotero/storage/KNM6IDPD/CURA_GeoVA_ECTQG2017.pptx},
  note = {00000 
cites: cura\_making\_2017}
}

@incollection{inselberg_parallel_1987,
  title = {Parallel Coordinates for Visualizing Multi-Dimensional Geometry},
  booktitle = {Computer {{Graphics}} 1987},
  publisher = {{Springer}},
  date = {1987},
  pages = {25--44},
  author = {Inselberg, Alfred and Dimsdale, Bernard},
  file = {/home/robin/Dropbox/Biblio/Inselberg_Dimsdale/Inselberg_Dimsdale_1987_Parallel_coordinates_for_visualizing.pdf},
  note = {01538}
}

@article{few_multivariate_2006,
  title = {Multivariate Analysis Using Parallel Coordinates},
  journaltitle = {Perceptual edge},
  date = {2006},
  pages = {1--9},
  author = {Few, Stephen},
  file = {/home/robin/Dropbox/Biblio/Few/Few_2006_Multivariate_analysis_using_parallel_coordinates.pdf},
  note = {00039}
}

@inproceedings{heinrich_state_2013,
  langid = {english},
  title = {State of the {{Art}} of {{Parallel Coordinates}}},
  url = {https://diglib.eg.org:443/handle/10.2312/conf.EG2013.stars.095-116},
  doi = {10/gd87qm},
  abstract = {This work presents a survey of the current state of the art of visualization techniques for parallel coordinates. It covers geometric models for constructing parallel coordinates and reviews methods for creating and understanding visual representations of parallel coordinates. The classification of these methods is based on a taxonomy that was established from the literature and is aimed at guiding researchers to find existing techniques and identifying white spots that require further research. The techniques covered in this survey are further related to an established taxonomy of knowledge-discovery tasks to support users of parallel coordinates in choosing a technique for their problem at hand. Finally, we discuss the challenges in constructing and understanding parallel-coordinates plots and provide some examples from different application domains.},
  eventtitle = {Eurographics ({{STARs}})},
  date = {2013},
  pages = {95--116},
  keywords = {Visual Analysis,Parallel coordinates},
  author = {Heinrich, Julian and Weiskopf, Daniel},
  file = {/home/robin/Dropbox/Biblio/Heinrich_Weiskopf/Heinrich_Weiskopf_2013_State_of_the_Art_of_Parallel_Coordinates.pdf}
}

@book{iannone_flexdashboard_2018,
  title = {Flexdashboard: {{R Markdown Format}} for {{Flexible Dashboards}}},
  url = {https://CRAN.R-project.org/package=flexdashboard},
  date = {2018},
  author = {Iannone, Richard and Allaire, Joseph J. and Borges, Barbara},
  note = {00000 
R package version 0.5.1.1}
}

@article{rivard_are_2004,
  title = {Are {{You Drowning}} in {{BI Reports}}? {{Using Analytical Dashboards}} to Cut through the Clutter},
  journaltitle = {DM Review, http://goo. gl/hle9Wc},
  date = {2004},
  author = {Rivard, Kurt and Cogswell, Doug},
  file = {/home/robin/Dropbox/Biblio/Rivard_Cogswell_2004_Are_You_Drowning_in_BI_Reports.pdf},
  note = {00000}
}

@book{few_information_2006,
  title = {Information {{Dashboard Design}}: {{The Effective Visual Communication}} of {{Data}}},
  isbn = {978-0-596-10016-2},
  shorttitle = {Information {{Dashboard Design}}},
  abstract = {Dashboards have become popular in recent years as uniquely powerful tools for communicating important information at a glance. Although dashboards are potentially powerful, this potential is rarely realized. The greatest display technology in the world won't solve this if you fail to use effective visual design. And if a dashboard fails to tell you precisely what you need to know in an instant, you'll never use it, even if it's filled with cute gauges, meters, and traffic lights. Don't let your investment in dashboard technology go to waste.This book will teach you the visual design skills you need to create dashboards that communicate clearly, rapidly, and compellingly. Information Dashboard Design will explain how to:Avoid the thirteen mistakes common to dashboard designProvide viewers with the information they need quickly and clearlyApply what we now know about visual perception to the visual presentation of informationMinimize distractions, cliches, and unnecessary embellishments that create confusionOrganize business information to support meaning and usabilityCreate an aesthetically pleasing viewing experienceMaintain consistency of design to provide accurate interpretationOptimize the power of dashboard technology by pairing it with visual effectivenessStephen Few has over 20 years of experience as an IT innovator, consultant, and educator. As Principal of the consultancy Perceptual Edge, Stephen focuses on data visualization for analyzing and communicating quantitative business information. He provides consulting and training services, speaks frequently at conferences, and teaches in the MBA program at the University of California in Berkeley. He is also the author of Show Me the Numbers: Designing Tables and Graphs to Enlighten. Visit his website at www.perceptualedge.com.},
  publisher = {{O'Reilly Media, Inc.}},
  date = {2006},
  author = {Few, Stephen},
  note = {00001}
}

@article{batty_perspective_2015,
  title = {A Perspective on City Dashboards},
  volume = {2},
  issn = {null},
  url = {https://doi.org/10.1080/21681376.2014.987540},
  doi = {10/gfw9mr},
  number = {1},
  journaltitle = {Regional Studies, Regional Science},
  date = {2015-01-01},
  pages = {29-32},
  keywords = {city dashboard},
  author = {Batty, Michael},
  file = {/home/robin/Dropbox/Biblio/Batty/Batty_2015_A_perspective_on_city_dashboards.pdf;/home/robin/Zotero/storage/T88TXKJT/21681376.2014.html},
  note = {00016}
}

@article{kitchin_knowing_2015,
  title = {Knowing and Governing Cities through Urban Indicators, City Benchmarking and Real-Time Dashboards},
  volume = {2},
  issn = {null},
  url = {https://doi.org/10.1080/21681376.2014.983149},
  doi = {10/gc92g7},
  abstract = {Since the mid-1990s a plethora of indicator projects have been developed and adopted by cities seeking to measure and monitor various aspects of urban systems. These have been accompanied by city benchmarking endeavours that seek to compare intra- and inter-urban performance. More recently, the data underpinning such projects have started to become more open to citizens, more real-time in nature generated through sensors and locative/social media, and displayed via interactive visualisations and dashboards that can be accessed via the internet. In this paper, we examine such initiatives arguing that they advance a narrowly conceived but powerful realist epistemology – the city as visualised facts – that is reshaping how managers and citizens come to know and govern cities. We set out how and to what ends indicator, benchmarking and dashboard initiatives are being employed by cities. We argue that whilst these initiatives often seek to make urban processes and performance more transparent and to improve decision making, they are also underpinned by a naive instrumental rationality, are open to manipulation by vested interests, and suffer from often unacknowledged methodological and technical issues. Drawing on our own experience of working on indicator and dashboard projects, we argue for a conceptual re-imaging of such projects as data assemblages – complex, politically-infused, socio-technical systems that, rather than reflecting cities, actively frame and produce them.},
  number = {1},
  journaltitle = {Regional Studies, Regional Science},
  date = {2015-01-01},
  pages = {6-28},
  keywords = {cities,benchmarking,City dashboard,dashboards,data assemblage,epistemology,governance,indicators,real-time},
  author = {Kitchin, Rob and Lauriault, Tracey P. and McArdle, Gavin},
  file = {/home/robin/Dropbox/Biblio/Kitchin_Lauriaultet-al/Kitchin_Lauriaultet-al_2015_Knowing_and_governing_cities_through_urban.pdf},
  note = {00169}
}

@inproceedings{roumpani_creating_2013,
  title = {Creating, Visualizing and Modelling the Realtime City},
  booktitle = {Proceedings of {{Hybrid City II}} ‘{{Subtle rEvolutions}}’ {{Conference}}},
  date = {2013},
  keywords = {City dashboard,CASA,⛔ No DOI found},
  author = {Roumpani, F. and O’Brien, O. and Hudson-Smith, A.},
  file = {/home/robin/Dropbox/Biblio/Roumpani_O’Brienet-al_2013_Creating,_visualizing_and_modelling_the_realtime_city.pdf},
  note = {00002}
}

@inproceedings{grignard_agent-based_2017,
  langid = {english},
  title = {Agent-{{Based Visualization}}: {{A Real}}-{{Time Visualization Tool Applied Both}} to {{Data}} and {{Simulation Outputs}}},
  shorttitle = {Agent-{{Based Visualization}}},
  abstract = {Information visualization is the study of interactive visual representations of abstract data to reinforce human cognition. Most existing visualization techniques are not suited to explore and understand datasets from heterogeneous and complex sources. Assuming that agent-based models properly represent the complexity of a real system, we propose to use an approach based on the deﬁnition of an agent-based model to facilitate visual representation of simulation outputs and complex data. These concepts have been implemented in the GAMA modeling and simulation platform, in which we developed a 3D immersive environment offering the user different points of view and ways to interact. We implemented models chosen for their properties to support a linear progression in terms of complexity to test their ﬂexibility, modularity, and adaptability. Finally, we demonstrate through the particular case of data visualization, how our approach allows us, in real time, to represent, clarify, or even discover dynamics and how that progress in terms of visualization can contribute, in turn, to improve the modeling of complex systems.},
  eventtitle = {Association for the {{Advancement}} of {{Artificial Intelligence}} 17},
  booktitle = {The {{AAAI}}-17 {{Workshop}} on   {{Human}}-{{Machine Collaborative Learning}}},
  date = {2017},
  keywords = {ABM,Visual Analysis,Bibliography,Review,Simulation,Simulation vizualisation,TO_READ,Visualisation workflow,⛔ No DOI found},
  author = {Grignard, Arnaud and Drogoul, Alexis},
  file = {/home/robin/Dropbox/Biblio/Grignard_Drogoul/Grignard_Drogoul_2017_Agent-Based_Visualization.pdf},
  note = {00002}
}

@article{roth_interactivity_2015,
  langid = {english},
  title = {Interactivity and {{Cartography}}: {{A Contemporary Perspective}} on {{User Interface}} and {{User Experience Design}} from {{Geospatial Professionals}}},
  url = {https://www.utpjournals.press/doi/abs/10.3138/cart.50.2.2427},
  doi = {10/gfw9mp},
  shorttitle = {Interactivity and {{Cartography}}},
  abstract = {ABSTRACT This article reports on a semi-structured interview study with 21 geospatial professionals to provide a contemporary snapshot of expert opinion on the design and use of interactive maps and map-based systems (treated together as “cartographic interfaces”). Interview questions were based on key themes regarding interaction discussed within cartography and across the related fields of human-computer interaction, information visualization, usability engineering, and visual analytics, enabling a comparison of the current states of science and practice regarding user interface (UI) and user experience (UX) design in cartography. The results are organized according to five broad topics germane to UI/UX design in cartography: (1) the meaning of cartographic interaction in both research and practice (what?), (2) the purpose of cartographic interaction and the value it provides (why?), (3) the times when interaction positively supports work/play and therefore should be provided (when?), (4) the way in whi...},
  journaltitle = {Cartographica: The International Journal for Geographic Information and Geovisualization},
  date = {2015-01-01},
  author = {Roth, Robert E.},
  file = {/home/robin/Dropbox/Biblio/Roth_2015_Interactivity_and_Cartography.pdf},
  note = {00016}
}

@incollection{fekete_infrastructure_2010,
  title = {Infrastructure},
  url = {https://hal.inria.fr/hal-00696814},
  abstract = {Supporting the strong demand in data storage, computation and interactive performances required by visual analytics applications is still a challenge. All currently existing visual analytics applications need to build their own specialised infrastructure for their speciﬁc problem. One of the most difficult issues of visual analytics is that it is both user driven and data-driven. It is user-driven because during the interactive steps of the analysis, the user is specifying algorithms and parameters to explore the data. It is also data-driven because new data is made available to the user at unpredictable times, such as when algorithms run or databases are updated. This chapter describes the landscape of software and hardware solutions to address this difficult issues.},
  booktitle = {Mastering the Information Age - {{Solving}} Problems with Visual Analytics},
  publisher = {{Eurographics Association}},
  date = {2010-11},
  pages = {87-108},
  author = {Fekete, Jean-Daniel},
  editor = {Association, Eurographics},
  file = {/home/robin/Dropbox/Biblio/Fekete/Fekete_2010_Infrastructure.pdf},
  note = {00178}
}

@article{dos_santos_gaining_2004,
  title = {Gaining Understanding of Multivariate and Multidimensional Data through Visualization},
  volume = {28},
  issn = {0097-8493},
  url = {http://www.sciencedirect.com/science/article/pii/S0097849304000251},
  doi = {10/cttwpw},
  abstract = {High dimensionality is a major challenge for data visualization. Parameter optimization problems require an understanding of the behaviour of the objective function in the n-dimensional space around the optimum—this is multidimensional visualization and is the traditional domain of scientific visualization. Large data tables require us to understand the relationship between attributes in the table—this is multivariate visualization and is an important aspect of information visualization. Common to both types of ‘high-dimensional’ visualization is a need to reduce the dimensionality for display. In this paper we present a uniform approach to the filtering of both multidimensional and multivariate data, to allow extraction of data subject to constraints on their position or value within an n-dimensional window, and on choice of dimensions for display. A simple example of understanding the trajectory of solutions from an optimization algorithm is given—this involves a combination of multidimensional and multivariate data.},
  number = {3},
  journaltitle = {Computers \& Graphics},
  shortjournal = {Computers \& Graphics},
  date = {2004-06-01},
  pages = {311-325},
  keywords = {Multidimensional,Multivariate,Reference model,Visualisation},
  author = {dos Santos, Selan and Brodlie, Ken},
  options = {useprefix=true},
  file = {/home/robin/Dropbox/Biblio/dos Santos_Brodlie/dos_Santos_Brodlie_2004_Gaining_understanding_of_multivariate_and.pdf},
  note = {00107}
}

@book{keim_mastering_2010,
  title = {Mastering the {{Information Age Solving Problems}} with {{Visual Analytics}}},
  publisher = {{Eurographics Association}},
  date = {2010},
  author = {Keim, Daniel and Kohlhammer, Jörn and Ellis, Geoffrey and Mansmann, Florian},
  file = {/home/robin/Dropbox/Biblio/Keim_Kohlhammeret-al/Keim_Kohlhammeret-al_2010_Mastering_the_Information_Age_Solving_Problems.pdf;/home/robin/Zotero/storage/UUBQD5QQ/Capture d’écran de 2018-09-05 14-44-15.png},
  note = {00019}
}

@inreference{noauthor_snowflake_2018,
  langid = {english},
  title = {Snowflake Schema},
  url = {https://en.wikipedia.org/w/index.php?title=Snowflake_schema&oldid=856363739},
  abstract = {In computing, a snowflake schema is a logical arrangement of tables in a multidimensional database such that the entity relationship diagram resembles a snowflake  shape. The snowflake schema is represented by centralized fact tables which are connected to multiple dimensions..  "Snowflaking" is a method of normalizing the dimension tables in a star schema. When it is completely normalized along all the dimension tables, the resultant structure resembles a snowflake with the fact table in the middle. The principle behind snowflaking is normalization of the dimension tables by removing low cardinality attributes and forming separate tables.The snowflake schema is similar to the star schema. However, in the snowflake schema, dimensions are normalized into multiple related tables, whereas the star schema's dimensions are denormalized with each dimension represented by a single table. A complex snowflake shape emerges when the dimensions of a snowflake schema are elaborate, having multiple levels of relationships, and the child tables have multiple parent tables ("forks in the road").},
  booktitle = {Wikipedia},
  date = {2018-08-24},
  file = {/home/robin/Zotero/storage/B5UE3R46/index.html},
  note = {00001 
Page Version ID: 856363739}
}

@inreference{noauthor_star_2018,
  langid = {english},
  title = {Star Schema},
  url = {https://en.wikipedia.org/w/index.php?title=Star_schema&oldid=853084099},
  abstract = {In computing, the star schema is the simplest style of data mart schema and is the approach most widely used to develop data warehouses and dimensional data marts. The star schema consists of one or more fact tables referencing any number of dimension tables. The star schema is an important special case of the snowflake schema, and is more effective for handling simpler queries.The star schema gets its name from the physical model's resemblance to a star shape with a fact table at its center and the dimension tables surrounding it representing the star's points.},
  booktitle = {Wikipedia},
  date = {2018-08-02},
  file = {/home/robin/Zotero/storage/MENE5EAY/index.html},
  note = {00001 
Page Version ID: 853084099}
}

@inproceedings{raasveldt_monetdblite_2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1805.08520},
  location = {{New York, NY, USA}},
  title = {{{MonetDBLite}}: {{An Embedded Analytical Database}}},
  isbn = {978-1-4503-4703-7},
  url = {http://arxiv.org/abs/1805.08520},
  doi = {10/gfw9mm},
  shorttitle = {{{MonetDBLite}}},
  abstract = {While traditional RDBMSes offer a lot of advantages, they require significant effort to setup and to use. Because of these challenges, many data scientists and analysts have switched to using alternative data management solutions. These alternatives, however, lack features that are standard for RDBMSes, e.g. out-of-core query execution. In this paper, we introduce the embedded analytical database MonetDBLite. MonetDBLite is designed to be both highly efficient and easy to use in conjunction with standard analytical tools. It can be installed using standard package managers, and requires no configuration or server management. It is designed for OLAP scenarios, and offers near-instantaneous data transfer between the database and analytical tools, all the while maintaining the transactional guarantees and ACID properties of a standard relational system. These properties make MonetDBLite highly suitable as a storage engine for data used in analytics, machine learning and classification tasks.},
  booktitle = {Proceedings of the 2018 {{International Conference}} on {{Management}} of {{Data}}},
  series = {{{SIGMOD}} '18},
  publisher = {{ACM}},
  date = {2018},
  pages = {1837--1838},
  keywords = {machine learning,embedded databases,olap},
  author = {Raasveldt, Mark and Mühleisen, Hannes},
  file = {/home/robin/Dropbox/Biblio/Raasveldt_2018_MonetDBLite.pdf},
  note = {00001}
}

@inproceedings{vermeij_monetdb_2008,
  title = {{{MonetDB}}, a Novel Spatial Columnstore Dbms},
  booktitle = {Academic {{Proceedings}} of the 2008 {{Free}} and {{Open Source}} for {{Geospatial}} ({{FOSS4G}}) {{Conference}}, {{OSGeo}}},
  date = {2008},
  pages = {193--199},
  keywords = {⛔ No DOI found},
  author = {Vermeij, Maarten and Quak, Wilko and Kersten, Martin and Nes, Niels},
  file = {/home/robin/Dropbox/Biblio/Vermeij_Quaket-al_2008_Monetdb,_a_novel_spatial_columnstore_dbms.pdf},
  note = {00015}
}

@inproceedings{root_mapd_2016,
  location = {{New York, NY, USA}},
  title = {{{MapD}}: {{A GPU}}-Powered {{Big Data Analytics}} and {{Visualization Platform}}},
  isbn = {978-1-4503-4282-7},
  url = {http://doi.acm.org/10.1145/2897839.2927468},
  doi = {10/gd7hg8},
  shorttitle = {{{MapD}}},
  abstract = {MapD, or "Massively Parallel Database", is a big data analytics platform that can query and visualize big data up to 100x faster than other systems. It leverages the massive parallelism of commodity GPUs to execute SQL queries over multi-billion row datasets with millisecond response times, and optionally render the results using the GPU's native graphics pipeline. Depending on the use case, MapD can be used as a standalone SQL database or as a data visualization suite by using its own visualization frontend (see Fig. 1) or by integrating it with other third-party toolkits.},
  booktitle = {{{ACM SIGGRAPH}} 2016 {{Talks}}},
  series = {{{SIGGRAPH}} '16},
  publisher = {{ACM}},
  date = {2016},
  pages = {73:1--73:2},
  keywords = {data visualization,analytics,CUDA,database,GPU,OpenGL},
  author = {Root, Christopher and Mostak, Todd},
  file = {/home/robin/Dropbox/Biblio/Root_Mostak_2016_MapD.pdf}
}

@online{elliott_how_2017,
  title = {How {{Page Load Time Affects Bounce Rate}} and {{Page Views}}},
  url = {https://www.section.io/blog/page-load-time-bounce-rate/},
  abstract = {How page speed affects bounce rate and number of pages viewed per session. Website performance impact on revenue.},
  date = {2017-08-10},
  author = {Elliott, Roxana},
  editora = {{Section.io}},
  editoratype = {collaborator},
  note = {cache: https://webcache.googleusercontent.com/search?q=cache:MLRdmxm76NEJ:https://www.section.io/blog/page-speed-bounce-rate/+\&cd=5\&hl=fr\&ct=clnk\&gl=fr\&client=ubuntu}
}

@online{patel_speed_2011,
  langid = {english},
  title = {Speed {{Is A Killer}}},
  url = {https://neilpatel.com/blog/speed-is-a-killer/},
  shorttitle = {Speed {{Is A Killer}}},
  abstract = {Why Decreasing Page Load Time Can Drastically Increase Conversions

Can the speed of your website really have that much of an effect on your sales? Even if your site isn’t loading too slowly, can it still be improved? And how does Google factor into all of this? You might be surprised.},
  journaltitle = {Neil Patel},
  urldate = {2018-09-02},
  date = {2011-05-10},
  author = {Patel, Neil},
  note = {00000}
}

@inproceedings{zeng_iolap_2016,
  location = {{New York, NY, USA}},
  title = {{{iOLAP}}: {{Managing Uncertainty}} for {{Efficient Incremental OLAP}}},
  isbn = {978-1-4503-3531-7},
  url = {http://doi.acm.org/10.1145/2882903.2915240},
  doi = {10/gfw9mn},
  shorttitle = {{{iOLAP}}},
  abstract = {The size of data and the complexity of analytics continue to grow along with the need for timely and cost-effective analysis. However, the growth of computation power cannot keep up with the growth of data. This calls for a paradigm shift from traditional batch OLAP processing model to an incremental OLAP processing model. In this paper, we propose iOLAP, an incremental OLAP query engine that provides a smooth trade-off between query accuracy and latency, and fulfills a full spectrum of user requirements from approximate but timely query execution to a more traditional accurate query execution. iOLAP enables interactive incremental query processing using a novel mini-batch execution model---given an OLAP query, iOLAP first randomly partitions the input dataset into smaller sets (mini-batches) and then incrementally processes through these mini-batches by executing a delta update query on each mini-batch, where each subsequent delta update query computes an update based on the output of the previous one. The key idea behind iOLAP is a novel delta update algorithm that models delta processing as an uncertainty propagation problem, and minimizes the recomputation during each subsequent delta update by minimizing the uncertainties in the partial (including intermediate) query results. We implement iOLAP on top of Apache Spark and have successfully demonstrated it at scale on over 100 machines. Extensive experiments on a multitude of queries and datasets demonstrate that iOLAP can deliver approximate query answers for complex OLAP queries orders of magnitude faster than traditional OLAP engines, while continuously delivering updates every few seconds.},
  booktitle = {Proceedings of the 2016 {{International Conference}} on {{Management}} of {{Data}}},
  series = {{{SIGMOD}} '16},
  publisher = {{ACM}},
  date = {2016},
  pages = {1347--1361},
  keywords = {approximate query processing,bootstrap,incremental,OLAP},
  author = {Zeng, Kai and Agarwal, Sameer and Stoica, Ion},
  file = {/home/robin/Dropbox/Biblio/Zeng_Agarwalet-al_2016_iOLAP.pdf},
  note = {00012}
}

@inproceedings{agarwal_blinkdb_2013,
  location = {{New York, NY, USA}},
  title = {{{BlinkDB}}: {{Queries}} with {{Bounded Errors}} and {{Bounded Response Times}} on {{Very Large Data}}},
  isbn = {978-1-4503-1994-2},
  url = {http://doi.acm.org/10.1145/2465351.2465355},
  doi = {10/bwrd},
  shorttitle = {{{BlinkDB}}},
  abstract = {In this paper, we present BlinkDB, a massively parallel, approximate query engine for running interactive SQL queries on large volumes of data. BlinkDB allows users to trade-off query accuracy for response time, enabling interactive queries over massive data by running queries on data samples and presenting results annotated with meaningful error bars. To achieve this, BlinkDB uses two key ideas: (1) an adaptive optimization framework that builds and maintains a set of multi-dimensional stratified samples from original data over time, and (2) a dynamic sample selection strategy that selects an appropriately sized sample based on a query's accuracy or response time requirements. We evaluate BlinkDB against the well-known TPC-H benchmarks and a real-world analytic workload derived from Conviva Inc., a company that manages video distribution over the Internet. Our experiments on a 100 node cluster show that BlinkDB can answer queries on up to 17 TBs of data in less than 2 seconds (over 200 x faster than Hive), within an error of 2-10\%.},
  booktitle = {Proceedings of the 8th {{ACM European Conference}} on {{Computer Systems}}},
  series = {{{EuroSys}} '13},
  publisher = {{ACM}},
  date = {2013},
  pages = {29--42},
  keywords = {database,estimation,query approximation,time limit},
  author = {Agarwal, Sameer and Mozafari, Barzan and Panda, Aurojit and Milner, Henry and Madden, Samuel and Stoica, Ion},
  file = {/home/robin/Dropbox/Biblio/Agarwal_Mozafariet-al/Agarwal_Mozafariet-al_2013_BlinkDB.pdf},
  note = {00474}
}

@book{bimonte_integration_2007,
  title = {Intégration de l'information Géographique Dans Les Entrepôts de Données et l'analyse En Ligne : De La Modélisation à La Visualisation},
  url = {http://www.theses.fr/2007ISAL0105},
  shorttitle = {Intégration de l'information Géographique Dans Les Entrepôts de Données et l'analyse En Ligne},
  abstract = {Les systèmes d’entrepôts de données et OLAP sont de solutions pour l’analyse décisionnelle. L’intégration des données spatiales dans l’ OLAP est un enjeu majeur. L’information géographique est très fréquemment présente dans les données, mais généralement sous-employée dans le processus décisionnel. Le couplage de systèmes OLAP et de Systèmes d’Informations Géographiques au sein de systèmes OLAP Spatial (SOLAP) est une voie prometteuse. La majorité des solutions SOLAP réduisent l’information géographique à la seule composante spatiale, limitant ainsi les capacités d’analyse du paradigme spatio-multidimensionnel. Nous proposons un modèle formel (GeoCube) et une algèbre associée, qui reformule les concepts du SOLAP afin d’introduire les aspects sémantiques et spatiaux de l’information géographique dans l’analyse multidimensionnelle. Cela se traduit par une modélisation des mesures sous forme d'objets géographiques, dans une vision complètement symétrique entre mesures et dimensions. Ainsi une mesure peut participer à une hiérarchie. Nous proposons une algèbre qui fournit les opérateurs de forage et de coupe, un opérateur qui permet d'intervertir mesure et dimension et des opérateurs de navigation au sein de la hiérarchie de mesures. Cette algèbre, grâce aux opérateurs qui modifient dynamiquement la structure de l’hypercube, permet de concilier analyse OLAP et analyse spatiale. Nous avons réalisé un prototype web conforme à GeoCube. Pour décrire nos solutions, nous utilisons des données environnementales de la lagune de Venise. Enfin, nous proposons un nouveau paradigme de visualisation et d’interaction pour l’analyse des mesures géographiques.},
  publisher = {{Lyon, INSA}},
  date = {2007-01-01},
  author = {Bimonte, Sandro},
  file = {/home/robin/Dropbox/Biblio/Bimonte/Bimonte_2007_Intégration_de_l'information_géographique_dans.pdf;/home/robin/Zotero/storage/NSC73M5E/2007ISAL0105.html},
  note = {00029}
}

@inproceedings{bimonte_towards_2005,
  location = {{New York, NY, USA}},
  title = {Towards a {{Spatial Multidimensional Model}}},
  isbn = {978-1-59593-162-7},
  url = {http://doi.acm.org/10.1145/1097002.1097009},
  doi = {10/cfn8w7},
  abstract = {Data warehouses and OLAP systems help to interactively analyze huge volume of data. This data, extracted from transactional databases, frequently contains spatial information which is useful for decision-making process. Integration of spatial data in multidimensional models leads to the concept of SOLAP (Spatial OLAP). Using a spatial measure as a geographical object, i.e. with geometric and descriptive attributes, raises problems regarding the aggregation operation in its semantic and implementation aspects. This paper shows the requirements for a multidimensional spatial data model and presents a multidimensional data model which is able to support complex objects as measures, inter-dependent attributes for measures and aggregation functions, use of ad-hoc aggregation functions and n to n relations between fact and dimension, in order to handle geographical data, according to its particular nature in an OLAP context.},
  booktitle = {Proceedings of the 8th {{ACM International Workshop}} on {{Data Warehousing}} and {{OLAP}}},
  series = {{{DOLAP}} '05},
  publisher = {{ACM}},
  date = {2005},
  pages = {39--46},
  keywords = {data modelling,multidimensional data,spatial data,spatial OLAP},
  author = {Bimonte, S. and Tchounikine, A. and Miquel, M.},
  file = {/home/robin/Dropbox/Biblio/Bimonte_Tchounikineet-al/Bimonte_Tchounikineet-al_2005_Towards_a_Spatial_Multidimensional_Model.pdf},
  note = {00075}
}

@inproceedings{zaamoune_new_2013,
  title = {A New Relational Spatial {{OLAP}} Approach for Multi-Resolution and Spatio-Multidimensional Analysis of Incomplete Field Data},
  booktitle = {{{ICEIS}} 2013 {{INSTICC International Conference}} on {{Enterprise Information Systems}}},
  date = {2013},
  pages = {p},
  keywords = {OLAP,SOLAP,⛔ No DOI found},
  author = {Zaamoune, Mehdi and Bimonte, Sandro and Pinet, François and Beaune, Philippe},
  file = {/home/robin/Dropbox/Biblio/Zaamoune_Bimonteet-al_2013_A_new_relational_spatial_OLAP_approach_for_multi-resolution_and.pdf},
  note = {00010}
}

@unpublished{cura_creer_2015,
  langid = {french},
  venue = {{Lyon}},
  title = {Créer des documents reproductibles et des applications web interactives d’analyse de données avec R : Knitr \& Shiny},
  url = {https://github.com/RCura/CafeMethodo},
  eventtitle = {EVS-ISIG : Café méthodo},
  date = {2015-10-06},
  author = {Cura, Robin},
  note = {00000 
http://umr5600.ish-lyon.cnrs.fr/20150610\_EVS-ISIG\_CafeMethodo}
}

@book{commenges_r_2014,
  langid = {french},
  location = {{Lyon}},
  title = {R et espace: Traitement de l’information géographique},
  edition = {Groupe ElementR},
  isbn = {979-10-92674-06-4},
  shorttitle = {R et espace},
  abstract = {Support technique pour l’utilisation des méthodes d’analyse de l’information géographique à l’aide du logiciel libre R. Concerne 
l’analyse spatiale, à savoir les méthodes mises en œuvre pour l’étude des organisations dans l’espace. Des exemples de cartes réalisées (densité de population, visualisation de voisinages, répartition par classe sociale, capacité des hôpitaux ...)},
  publisher = {{Framabook}},
  date = {2014},
  keywords = {Analyse des données,Cartographie,Géographie,Infographie,Logiciel,Manuel,Statistique},
  author = {Commenges, Hadrien and Beauguitte, Laurent and Buard, Elodie and Cura, Robin and Le Néchet, Florent and Le Texier, Marion and Mathian, Hélene and Rey-Coyrehourcq, Sébastien}
}

@online{plotly_introducing_2017,
  title = {Introducing {{Dash}}},
  url = {https://medium.com/@plotlygraphs/introducing-dash-5ecf7191b503},
  abstract = {Create Reactive Web Apps in pure Python},
  journaltitle = {plotly},
  urldate = {2018-09-12},
  date = {2017-06-21T14:53:12.308Z},
  author = {Plotly},
  note = {00000}
}

@article{bezanson_julia_2014,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1411.1607},
  primaryClass = {cs},
  title = {Julia: {{A Fresh Approach}} to {{Numerical Computing}}},
  url = {http://arxiv.org/abs/1411.1607},
  shorttitle = {Julia},
  abstract = {Bridging cultures that have often been distant, Julia combines expertise from the diverse fields of computer science and computational science to create a new approach to numerical computing. Julia is designed to be easy and fast. Julia questions notions generally held as "laws of nature" by practitioners of numerical computing: 1. High-level dynamic programs have to be slow. 2. One must prototype in one language and then rewrite in another language for speed or deployment, and 3. There are parts of a system for the programmer, and other parts best left untouched as they are built by the experts. We introduce the Julia programming language and its design --- a dance between specialization and abstraction. Specialization allows for custom treatment. Multiple dispatch, a technique from computer science, picks the right algorithm for the right circumstance. Abstraction, what good computation is really about, recognizes what remains the same after differences are stripped away. Abstractions in mathematics are captured as code through another technique from computer science, generic programming. Julia shows that one can have machine performance without sacrificing human convenience.},
  date = {2014-11-06},
  keywords = {Computer Science - Mathematical Software},
  author = {Bezanson, Jeff and Edelman, Alan and Karpinski, Stefan and Shah, Viral B.},
  file = {/home/robin/Dropbox/Biblio/Bezanson_Edelmanet-al/Bezanson_Edelmanet-al_2014_Julia.pdf;/home/robin/Zotero/storage/JZRF2J39/1411.html},
  note = {00550}
}

@software{gowda_escher_2018,
  title = {Escher - {{Composable Web UIs}} in {{Julia}}.},
  url = {https://github.com/JuliaGizmos/Escher.jl},
  organization = {{JuliaGizmos}},
  date = {2018-09-11T02:10:22Z},
  author = {Gowda, Shashi},
  origdate = {2014-11-20T11:54:05Z},
  note = {00000}
}

@article{chang_shiny_2015,
  title = {Shiny: Web Application Framework for {{R}}},
  volume = {1},
  shorttitle = {Shiny},
  number = {4},
  journaltitle = {R package version 0.11},
  date = {2015},
  pages = {106},
  keywords = {⛔ No DOI found},
  author = {Chang, Winston and Cheng, Joe and Allaire, Joseph J. and Xie, Yihui and McPherson, Jonathan},
  note = {00553}
}

@book{shneiderman_designing_2004,
  title = {Designing the {{User Interface}}: {{Strategies}} for {{Effective Human}}-{{Computer Interaction}} (4th {{Edition}})},
  isbn = {978-0-321-19786-3},
  shorttitle = {Designing the {{User Interface}}},
  publisher = {{Pearson Addison Wesley}},
  date = {2004},
  keywords = {Visual Analysis,design,IHM},
  author = {Shneiderman, Ben and Plaisant, Catherine},
  file = {/home/robin/Dropbox/Biblio/Shneiderman_Plaisant_2004_Designing_the_User_Interface.pdf},
  note = {00000}
}

@book{ware_information_2012,
  title = {Information Visualization: Perception for Design},
  shorttitle = {Information Visualization},
  publisher = {{Elsevier}},
  date = {2012},
  author = {Ware, Colin},
  file = {/home/robin/Dropbox/Biblio/Ware_2012_Information_visualization.pdf;/home/robin/Zotero/storage/HHZQRUZG/books.html},
  note = {05270}
}

@inproceedings{forch_are_2017,
  langid = {english},
  title = {Are 100 Ms {{Fast Enough}}? {{Characterizing Latency Perception Thresholds}} in {{Mouse}}-{{Based Interaction}}},
  isbn = {978-3-319-58475-1},
  shorttitle = {Are 100 Ms {{Fast Enough}}?},
  abstract = {The claim that 100 ms system latency is fast enough for an optimal interaction with highly interactive computer systems has been challenged by several studies demonstrating that users are able to perceive latencies well below the 100 ms mark. Although a high amount of daily computer interactions is still characterized by mouse-based interaction, to date only few studies about latency perception thresholds have employed a corresponding interaction paradigm. Therefore, we determined latency perception thresholds in a mouse-based computer interaction task. We also tested whether user characteristics, such as experience with latency in computer interaction and interaction styles, might be related to inter-individual differences in latency perception thresholds, as results of previous studies indicate that there is considerable inter-individual variance in latency perception thresholds. Our results show that latency perception thresholds for a simple mouse-based computer interaction lie in the range of 60 ms and that inter-individual differences in latency perception can be related to user characteristics.},
  booktitle = {Engineering {{Psychology}} and {{Cognitive Ergonomics}}: {{Cognition}} and {{Design}}},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  publisher = {{Springer International Publishing}},
  date = {2017},
  pages = {45-56},
  keywords = {Human-computer interaction,Latency,Latency perception,Mouse-based interaction,System response time},
  author = {Forch, Valentin and Franke, Thomas and Rauh, Nadine and Krems, Josef F.},
  editor = {Harris, Don},
  file = {/home/robin/Dropbox/Biblio/Forch_Frankeet-al/Forch_Frankeet-al_2017_Are_100_ms_Fast_Enough.pdf},
  note = {00001}
}

@inproceedings{mackenzie_lag_1993,
  location = {{New York, NY, USA}},
  title = {Lag {{As}} a {{Determinant}} of {{Human Performance}} in {{Interactive Systems}}},
  isbn = {978-0-89791-575-5},
  url = {http://doi.acm.org/10.1145/169059.169431},
  doi = {10/dr7rj6},
  abstract = {The sources of lag (the delay between input action and output response) and its effects on human performance are discussed. We measured the effects in a study of target acquisition using the classic Fitts' law paradigm with the addition of four lag conditions. At the highest lag tested (225 ms), movement times and error rates increased by 64\% and 214\% respectively, compared to the zero lag condition. We propose a model according to which lag should have a multiplicative effect on Fitts' index of difficulty. The model accounts for 94\% of the variance and is better than alternative models which propose only an additive effect for lag. The implications for the design of virtual reality systems are discussed.},
  booktitle = {Proceedings of the {{INTERACT}} '93 and {{CHI}} '93 {{Conference}} on {{Human Factors}} in {{Computing Systems}}},
  series = {{{CHI}} '93},
  publisher = {{ACM}},
  date = {1993},
  pages = {488--493},
  keywords = {feedback delay,Fitts' law,human performance modeling,lag,speed-accuracy tradeoff,virtual reality},
  author = {MacKenzie, I. Scott and Ware, Colin},
  file = {/home/robin/Dropbox/Biblio/MacKenzie_Ware/MacKenzie_Ware_1993_Lag_As_a_Determinant_of_Human_Performance_in.pdf},
  note = {00442}
}

@article{fotheringham_trends_1999,
  langid = {english},
  title = {Trends in Quantitative Methods {{III}}: Stressing the Visual},
  volume = {23},
  issn = {0309-1325},
  url = {https://doi.org/10.1191/030913299667756016},
  doi = {10/c549jd},
  shorttitle = {Trends in Quantitative Methods {{III}}},
  number = {4},
  journaltitle = {Progress in Human Geography},
  shortjournal = {Progress in Human Geography},
  date = {1999-12-01},
  pages = {597-606},
  keywords = {GWR,parallel coordinates,spatial visualisation},
  author = {Fotheringham, A. Stewart},
  file = {/home/robin/Dropbox/Biblio/Fotheringham/Fotheringham_1999_Trends_in_quantitative_methods_III.pdf},
  note = {00038}
}

@article{maceachren_geovisualization_2004,
  title = {Geovisualization for {{Knowledge Construction}} and {{Decision Support}}},
  volume = {24},
  issn = {0272-1716},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3181162/},
  number = {1},
  journaltitle = {IEEE computer graphics and applications},
  shortjournal = {IEEE Comput Graph Appl},
  date = {2004},
  pages = {13-17},
  author = {MacEachren, Alan M. and Gahegan, Mark and Pike, William and Brewer, Isaac and Cai, Guoray and Lengerich, Eugene and Hardisty, Frank},
  file = {/home/robin/Dropbox/Biblio/MacEachren_Gaheganet-al/MacEachren_Gaheganet-al_2004_Geovisualization_for_Knowledge_Construction_and.pdf},
  eprinttype = {pmid},
  eprint = {15384662},
  pmcid = {PMC3181162},
  note = {00256}
}

@article{coltekin_geovisualization_2018,
  title = {Geovisualization},
  volume = {2018},
  issn = {25772848},
  url = {https://gistbok.ucgis.org/bok-topics/geovisualization},
  doi = {10.22224/gistbok/2018.2.6},
  number = {Q2},
  journaltitle = {Geographic Information Science \& Technology Body of Knowledge},
  date = {2018-04-01},
  author = {Çöltekin, Arzu and Janetzko, Halldór and Fabrikant, Sara},
  file = {/home/robin/Dropbox/Biblio/Çöltekin_Janetzkoet-al/Çöltekin_Janetzkoet-al_2018_Geovisualization.pdf}
}

@article{wickham_dplyr_2015,
  title = {Dplyr: {{A}} Grammar of Data Manipulation},
  volume = {3},
  shorttitle = {Dplyr},
  journaltitle = {R package version 0.4},
  date = {2015},
  keywords = {⛔ No DOI found},
  author = {Wickham, Hadley and Francois, Romain and Henry, Lionel and Müller, K.},
  note = {00547}
}

@book{tufte_visual_2001,
  langid = {Anglais},
  location = {{Cheshire, Conn}},
  title = {The {{Visual Display}} of {{Quantitative Information}}},
  edition = {2nd edition},
  isbn = {978-0-9613921-4-7},
  pagetotal = {190},
  publisher = {{Graphics Press USA}},
  date = {2001-01-31},
  author = {Tufte, Edward R.},
  file = {/home/robin/Dropbox/Biblio/Tufte/Tufte_2001_The_Visual_Display_of_Quantitative_Information.pdf},
  note = {11580}
}

@book{epstein_growing_1996,
  langid = {Anglais},
  location = {{Washington, D.C}},
  title = {Growing {{Artificial Societies}}: {{Social Science}} from the {{Bottom Up}}},
  isbn = {978-0-262-05053-1},
  shorttitle = {Growing {{Artificial Societies}}},
  pagetotal = {228},
  publisher = {{MIT Press}},
  date = {1996-11-29},
  author = {Epstein, Joshua M. and Axtell, Robert L.},
  note = {05176}
}

@inproceedings{reynolds_flocks_1987,
  location = {{New York, NY, USA}},
  title = {Flocks, {{Herds}} and {{Schools}}: {{A Distributed Behavioral Model}}},
  isbn = {978-0-89791-227-3},
  url = {http://doi.acm.org/10.1145/37401.37406},
  doi = {10/chhdjr},
  shorttitle = {Flocks, {{Herds}} and {{Schools}}},
  abstract = {The aggregate motion of a flock of birds, a herd of land animals, or a school of fish is a beautiful and familiar part of the natural world. But this type of complex motion is rarely seen in computer animation. This paper explores an approach based on simulation as an alternative to scripting the paths of each bird individually. The simulated flock is an elaboration of a particle systems, with the simulated birds being the particles. The aggregate motion of the simulated flock is created by a distributed behavioral model much like that at work in a natural flock; the birds choose their own course. Each simulated bird is implemented as an independent actor that navigates according to its local perception of the dynamic environment, the laws of simulated physics that rule its motion, and a set of behaviors programmed into it by the "animator." The aggregate motion of the simulated flock is the result of the dense interaction of the relatively simple behaviors of the individual simulated birds.},
  booktitle = {Proceedings of the 14th {{Annual Conference}} on {{Computer Graphics}} and {{Interactive Techniques}}},
  series = {{{SIGGRAPH}} '87},
  publisher = {{ACM}},
  date = {1987},
  pages = {25--34},
  author = {Reynolds, Craig W.},
  file = {/home/robin/Dropbox/Biblio/Reynolds_1987_Flocks,_Herds_and_Schools.pdf},
  note = {10133}
}

@book{wilkinson_grammar_2006,
  title = {The Grammar of Graphics},
  publisher = {{Springer Science \& Business Media}},
  date = {2006},
  author = {Wilkinson, Leland},
  file = {/home/robin/Dropbox/Biblio/Wilkinson/Wilkinson_2006_The_grammar_of_graphics.pdf;/home/robin/Zotero/storage/MNLH55YZ/books.html},
  origdate = {1999},
  note = {01037}
}

@article{wickham_tidyverse_2017,
  title = {Tidyverse: {{Easily}} Install and Load ’tidyverse’ Packages},
  volume = {1},
  shorttitle = {Tidyverse},
  number = {1},
  journaltitle = {R package version},
  date = {2017},
  keywords = {⛔ No DOI found},
  author = {Wickham, Hadley},
  note = {00113}
}

@book{wickham_ggplot2_2016,
  title = {Ggplot2: Elegant Graphics for Data Analysis},
  shorttitle = {Ggplot2},
  publisher = {{Springer}},
  date = {2016},
  author = {Wickham, Hadley},
  file = {/home/robin/Zotero/storage/FM4NFEP7/books.html},
  note = {00082}
}

@report{mortier_human-data_2014,
  langid = {english},
  location = {{Rochester, NY}},
  title = {Human-{{Data Interaction}}: {{The Human Face}} of the {{Data}}-{{Driven Society}}},
  url = {https://papers.ssrn.com/abstract=2508051},
  shorttitle = {Human-{{Data Interaction}}},
  abstract = {The increasing generation and collection of personal data has created a complex ecosystem, often collaborative but sometimes combative, around companies and individuals engaging in the use of these data. We propose that the interactions between these agents warrants a new topic of study: Human-Data Interaction (HDI). In this paper we discuss how HDI sits at the intersection of various disciplines, including computer science, statistics, sociology, psychology and behavioural economics. We expose the challenges that HDI raises, organised into three core themes of legibility, agency and negotiability, and we present the HDI agenda to open up a dialogue amongst interested parties in the personal and big data ecosystems.},
  number = {ID 2508051},
  institution = {{Social Science Research Network}},
  type = {SSRN Scholarly Paper},
  urldate = {2018-09-17},
  date = {2014-10-01},
  keywords = {Economics,HCI,Human Data,Personal Data,Privacy,Psychology,Visualisation},
  author = {Mortier, Richard and Haddadi, Hamed and Henderson, Tristan and McAuley, Derek and Crowcroft, Jon}
}

@book{healy_data_2018,
  langid = {english},
  location = {{S.l.}},
  title = {Data {{Visualization}}: {{A Practical Introduction}}},
  isbn = {978-0-691-18161-5},
  url = {http://socviz.co/},
  shorttitle = {Data {{Visualization}}},
  abstract = {An accessible primer on how to create effective graphics from dataThis book provides students and researchers a hands-on introduction to the principles and practice of data visualization. It explains what makes some graphs succeed while others fail, how to make high-quality figures from data using powerful and reproducible methods, and how to think about data visualization in an honest and effective way.Data Visualization builds the reader’s expertise in ggplot2, a versatile visualization library for the R programming language. Through a series of worked examples, this accessible primer then demonstrates how to create plots piece by piece, beginning with summaries of single variables and moving on to more complex graphics. Topics include plotting continuous and categorical variables; layering information on graphics; producing effective “small multiple” plots; grouping, summarizing, and transforming data for plotting; creating maps; working with the output of statistical models; and refining plots to make them more comprehensible.Effective graphics are essential to communicating ideas and a great way to better understand data. This book provides the practical skills students and practitioners need to visualize quantitative data and get the most out of their research findings.Provides hands-on instruction using R and ggplot2Shows how the “tidyverse” of data analysis tools makes working with R easier and more consistentIncludes a library of data sets, code, and functions},
  pagetotal = {304},
  publisher = {{Princeton University Press}},
  date = {2018-12-18},
  author = {Healy, Kieran},
  note = {00000}
}

@inproceedings{elmqvist_embodied_2011,
  title = {Embodied Human-Data Interaction},
  booktitle = {{{ACM CHI}} 2011 {{Workshop}} “{{Embodied Interaction}}: {{Theory}} and {{Practice}} in {{HCI}}},
  date = {2011},
  pages = {104--107},
  keywords = {⛔ No DOI found},
  author = {Elmqvist, Niklas},
  note = {00016}
}

@article{bimonte_when_2010,
  langid = {english},
  title = {When {{Spatial Analysis Meets OLAP}}: {{Multidimensional Model}} and {{Operators}}},
  volume = {6},
  issn = {1548-3924 DOI: 10.4018/jdwm.2010100103},
  url = {https://www.igi-global.com/article/when-spatial-analysis-meets-olap/46942},
  doi = {10.4018/jdwm.2010100103},
  shorttitle = {When {{Spatial Analysis Meets OLAP}}},
  abstract = {When Spatial Analysis Meets OLAP: Multidimensional Model and Operators: 10.4018/jdwm.2010100103: Introducing spatial data into multidimensional models leads to the concept of Spatial OLAP (SOLAP). Existing SOLAP models do not completely integrate the},
  number = {4},
  journaltitle = {International Journal of Data Warehousing and Mining (IJDWM)},
  shortjournal = {IJDWM},
  date = {2010-10-01},
  pages = {33-60},
  author = {Bimonte, Sandro and Tchounikine, Anne and Miquel, Maryvonne and Pinet, François},
  file = {/home/robin/Dropbox/Biblio/Bimonte_Tchounikineet-al/Bimonte_Tchounikineet-al_2010_When_Spatial_Analysis_Meets_OLAP.pdf}
}

@book{pumain_urban_2017,
  langid = {english},
  title = {Urban {{Dynamics}} and {{Simulation Models}}},
  isbn = {978-3-319-46495-4},
  url = {//www.springer.com/us/book/9783319464954},
  abstract = {This monograph presents urban simulation methods that help in better understanding urban dynamics. Over historical times, cities have progressively absorbed a larger part of human population and will concentrate three quarters of humankind before the end of the century. This “urban transition” that has totally transformed the way we inhabit the planet is globally understood in its socio-economic rationales but is less frequently questioned as a spatio-temporal process. However, the cities, because they are intrinsically linked in a game of competition for resources and development, self organize in “systems of cities” where their future becomes more and more interdependent. The high frequency and intensity of interactions between cities explain that urban systems all over the world exhibit large similarities in their hierarchical and functional structure and rather regular dynamics. They are complex systems whose emergence, structure and further evolution are widely governed by the multiple kinds of interaction that link the various actors and institutions investing in cities their efforts, capital, knowledge and intelligence. Simulation models that reconstruct this dynamics may help in better understanding it and exploring future plausible evolutions of urban systems. This would provide better insight about how societies can manage the ecological transition at local, regional and global scales. The author has developed a series of instruments that greatly improve the techniques of validation for such models of social sciences that can be submitted to many applications in a variety of geographical situations. Examples are given for several BRICS countries, Europe and United States. The target audience primarily comprises research experts in the field of urban dynamics, but the book may also be beneficial for graduate students.},
  series = {Lecture {{Notes}} in {{Morphogenesis}}},
  publisher = {{Springer International Publishing}},
  date = {2017},
  author = {Pumain, Denise and Reuillon, Romain},
  file = {/home/robin/Dropbox/Biblio/Pumain_Reuillon_2017_Urban_Dynamics_and_Simulation_Models.pdf}
}

@article{cura_timelineedb_2017,
  langid = {french},
  title = {« TimeLineEDB », application web d'exploration interactive de données de géolocalisation},
  volume = {120},
  url = {https://halshs.archives-ouvertes.fr/halshs-01935702/document},
  abstract = {Contexte D'après le dernier Baromètre du Numérique (Arcep, 2016), près de deux Français sur trois possèdent un smartphone, et parmi eux, une forte part l'utilise fréquemment pour « chercher un restaurant, un bar, un musée ou un magasin à partir d'une application [où ils sont géolocalisés] ». Les principaux systèmes d'exploitation mobiles (Apple iOS et Google Android), après accord de l'utilisateur 1 , collectent de manière systématique un historique des localisations enregistrées sur le smartphone. Cela produit à terme, pour chaque utilisateur de ces systèmes, des données spatio-temporelles massives. Elles peuvent être consultées sur les smartphones dans tous les cas, mais seul Android permet leur visualisation externe ainsi que leur export, via le service Google Timeline. Dans la lignée des pratiques de « quantified self », nous avons créé un outil d'exploration de ces données, TimeLine Exploratory DashBoard (TimeLineEDB), afin de permettre à chacun de mesurer l'ampleur de ces informations collectées, et, au moyen d'une interface d'analyse intuitive, de rendre compte de la forte capacité intrusive que ces données peuvent conférer. Sur le plan de la géovisualisation, nous souhaitons ici montrer que face à une masse de données, les méthodes de représentation et d'interrogation les plus simples peuvent suffire à extraire de précieuses informations et synthèses des caractéristiques et comportements de mobilité d'un individu.},
  number = {2015/4},
  journaltitle = {M@ppemonde},
  urldate = {2018-12-21},
  date = {2017},
  keywords = {⛔ No DOI found},
  author = {Cura, Robin},
  file = {/home/robin/Dropbox/Biblio/Cura/Cura_2017_«_TimeLineEDB_»,_application_web_d'exploration.pdf},
  note = {00000}
}

@inproceedings{akbar_data_2018,
  title = {Data {{Analytics Enhanced Data Visualization}} and {{Interrogation}} with {{Parallel Coordinates Plots}}},
  doi = {10/gfw9pb},
  abstract = {Parallel coordinates plots (PCPs) suffer from curse of dimensionality when used with larger multidimensional datasets. Curse of dimentionality results in clutter which hides important visual data trends among coordinates. A number of solutions to address this problem have been proposed including filtering, aggregation, and dimension reordering. These solutions, however, have their own limitations with regard to exploring relationships and trends among the coordinates in PCPs. Correlation based coordinates reordering techniques are among the most popular and have been widely used in PCPs to reduce clutter, though based on the conducted experiments, this research has identified some of their limitations. To achieve better visualization with reduced clutter, we have proposed and evaluated dimensions reordering approach based on minimization of the number of crossing pairs. In the last step, k-means clustering is combined with reordered coordinates to highlight key trends and patterns. The conducted comparative analysis have shown that minimum crossings pairs approach performed much better than other applied techniques for coordinates reordering, and when combined with k-means clustering, resulted in better visualization with significantly reduced clutter.},
  eventtitle = {2018 26th {{International Conference}} on {{Systems Engineering}} ({{ICSEng}})},
  booktitle = {2018 26th {{International Conference}} on {{Systems Engineering}} ({{ICSEng}})},
  date = {2018-12},
  pages = {1-7},
  keywords = {Data visualization,Data analysis,data visualization,Market research,parallel coordinates,clustering,Clutter,correlation,Correlation,data analytics,Databases,Visualization},
  author = {Akbar, M. S. and Gabrys, B.},
  file = {/home/robin/Dropbox/Biblio/Akbar_Gabrys/Akbar_Gabrys_2018_Data_Analytics_Enhanced_Data_Visualization_and.pdf}
}

@article{yu_visflow_2017,
  title = {{{VisFlow}} - {{Web}}-Based {{Visualization Framework}} for {{Tabular Data}} with a {{Subset Flow Model}}},
  volume = {23},
  issn = {1077-2626},
  url = {http://bowenyu.me/publications},
  doi = {10/gfw9n9},
  abstract = {Data flow systems allow the user to design a flow diagram that specifies the relations between system components which process, filter or visually present the data. Visualization systems may benefit from user-defined data flows as an analysis typically consists of rendering multiple plots on demand and performing different types of interactive queries across coordinated views. In this paper, we propose VisFlow, a web-based visualization framework for tabular data that employs a specific type of data flow model called the subset flow model. VisFlow focuses on interactive queries within the data flow, overcoming the limitation of interactivity from past computational data flow systems. In particular, VisFlow applies embedded visualizations and supports interactive selections, brushing and linking within a visualization-oriented data flow. The model requires all data transmitted by the flow to be a data item subset (i.e. groups of table rows) of some original input table, so that rendering properties can be assigned to the subset unambiguously for tracking and comparison. VisFlow features the analysis flexibility of a flow diagram, and at the same time reduces the diagram complexity and improves usability. We demonstrate the capability of VisFlow on two case studies with domain experts on real-world datasets showing that VisFlow is capable of accomplishing a considerable set of visualization and analysis tasks. The VisFlow system is available as open source on GitHub.},
  number = {1},
  journaltitle = {IEEE Transactions on Visualization and Computer Graphics},
  date = {2017-01},
  pages = {251-260},
  keywords = {Data visualization,data visualisation,Data analysis,Computational modeling,Data models,analysis flexibility,computational complexity,computational data flow systems,data flow,diagram complexity,flow diagram,GitHub,interactive queries,interactive selections,Internet,Joining processes,Pipelines,public domain software,query processing,rendering (computer graphics),Rendering (computer graphics),rendering properties,subset flow model,tabular data,user-defined data flows,VisFlow,Visualization framework,visualization-oriented data flow,Web-based visualization framework},
  author = {Yu, B. and Silva, C. T.},
  file = {/home/robin/Dropbox/Biblio/Yu_Silva_2017_VisFlow_-_Web-based_Visualization_Framework_for_Tabular_Data_with_a_Subset_Flow.pdf}
}

@article{raseman_parasol_2019,
  title = {Parasol: An Open Source, Interactive Parallel Coordinates Library for Multi-Objective Decision Making},
  volume = {116},
  issn = {1364-8152},
  url = {http://www.sciencedirect.com/science/article/pii/S1364815219301148},
  doi = {10/gfw9n2},
  shorttitle = {Parasol},
  abstract = {This paper introduces Parasol—an open source, interactive visualization library to support the development of web applications for multi-objective decision making. Multi-objective optimization is a popular way to explore competing objectives in environmental management problems. Interactive visualizations allow stakeholders to explore and gain insights about the large, high-dimensional datasets produced by multi-objective optimization. Among visualization methods, parallel coordinates are well-suited for this task. However, current software and open source libraries have limited support for these plots. The Parasol library described in this work provides developers with the building blocks to create sharable, interactive parallel coordinates web applications. Moreover, by incorporating state of the art clutter reduction techniques—such as clustering, linking, brushing, marking, and bundling—Parasol improves upon traditional parallel coordinates visualizations. We demonstrate the benefit of such features through simple examples and by exploring a real-world water resources problem commonly used in multi-objective optimization literature.},
  journaltitle = {Environmental Modelling \& Software},
  shortjournal = {Environmental Modelling \& Software},
  date = {2019-06-01},
  pages = {153-163},
  keywords = {Decision making,Parallel coordinates,Visualization,Optimization,Web applications},
  author = {Raseman, William J. and Jacobson, Joshuah and Kasprzyk, Joseph R.},
  file = {/home/robin/Dropbox/Biblio/Raseman_Jacobsonet-al/Raseman_Jacobsonet-al_2019_Parasol.pdf}
}

@inproceedings{shneiderman1996eyes,
  title = {The Eyes Have It: A Task by Data Type Taxonomy for Information Visualizations},
  doi = {10/fwdq26},
  booktitle = {Proceedings 1996 {{IEEE Symposium}} on {{Visual Languages}}},
  date = {1996},
  pages = {336-343},
  author = {Shneiderman, Ben},
  file = {/home/robin/Dropbox/Biblio/Shneiderman/Shneiderman_1996_The_eyes_have_it.pdf},
  organization = {{IEEE}}
}

@online{pandre_charts_2011,
  langid = {english},
  title = {Charts and Their {{Dimensionality}}},
  url = {https://apandre.wordpress.com/dataviews/dimensionality/},
  abstract = {I blogged here~about the~Dimensionality of Visible Data and people kept asking me if I have a collection of examples of different charts and comments on each of their Dimensionality and may be a ~f…},
  journaltitle = {Data Visualization},
  date = {2011-03-06},
  author = {Pandre, Andrew}
}


