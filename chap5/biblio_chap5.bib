
@article{schelling_dynamic_1971,
  title = {Dynamic Models of Segregation},
  volume = {1},
  number = {2},
  journaltitle = {Journal of mathematical sociology},
  date = {1971},
  pages = {143--186},
  keywords = {\#nosource},
  author = {Schelling, Thomas C},
  note = {04241}
}

@article{cura_timelineedb_2017,
  title = {"{{TimeLineEDB}}", Application Web d'exploration Interactive de Donn√©es de G√©olocalisation},
  volume = {120},
  number = {2015/4},
  journaltitle = {M@ppemonde},
  date = {2017},
  keywords = {\#nosource,üîçNo DOI found},
  author = {Cura, Robin},
  note = {00000}
}

@thesis{amirpour_amraii_human-data_2018,
  title = {Human-{{Data Interaction}} in Large and {{High}}-{{Dimensional Data}}},
  institution = {{University of Pittsburgh}},
  type = {PhD Thesis},
  date = {2018},
  author = {Amirpour Amraii, Saman},
  file = {/home/robin/Dropbox/Biblio/Amirpour_Amraii_2018_Human-Data_Interaction_in_large_and_High-Dimensional_Data.pdf},
  note = {00000}
}

@article{roth_interactive_2013,
  title = {Interactive Maps: {{What}} We Know and What We Need to Know},
  issn = {1948-660X},
  url = {http://www.josis.org/index.php/josis/article/view/105},
  doi = {10.5311/JOSIS.2013.6.105},
  shorttitle = {Interactive Maps},
  number = {6},
  journaltitle = {Journal of Spatial Information Science},
  urldate = {2018-02-27},
  date = {2013-06-15},
  author = {Roth, Robert E.},
  file = {/home/robin/Dropbox/Biblio/Roth_2013_Interactive_maps.pdf},
  note = {00099}
}

@article{roth_interactivity_2015,
  langid = {english},
  title = {Interactivity and {{Cartography}}: {{A Contemporary Perspective}} on {{User Interface}} and {{User Experience Design}} from {{Geospatial Professionals}}},
  url = {https://www.utpjournals.press/doi/abs/10.3138/cart.50.2.2427},
  doi = {10.3138/cart.50.2.2427},
  shorttitle = {Interactivity and {{Cartography}}},
  abstract = {ABSTRACT This article reports on a semi-structured interview study with 21 geospatial professionals to provide a contemporary snapshot of expert opinion on the design and use of interactive maps and map-based systems (treated together as ‚Äúcartographic interfaces‚Äù). Interview questions were based on key themes regarding interaction discussed within cartography and across the related fields of human-computer interaction, information visualization, usability engineering, and visual analytics, enabling a comparison of the current states of science and practice regarding user interface (UI) and user experience (UX) design in cartography. The results are organized according to five broad topics germane to UI/UX design in cartography: (1) the meaning of cartographic interaction in both research and practice (what?), (2) the purpose of cartographic interaction and the value it provides (why?), (3) the times when interaction positively supports work/play and therefore should be provided (when?), (4) the way in whi...},
  journaltitle = {Cartographica: The International Journal for Geographic Information and Geovisualization},
  urldate = {2018-04-12},
  date = {2015-01-01},
  author = {Roth, Robert E.},
  file = {/home/robin/Dropbox/Biblio/Roth_2015_Interactivity_and_Cartography.pdf;/home/robin/.mozilla/firefox/2czgwusc.default/zotero/storage/3ZPERZYQ/cart.50.2.html},
  note = {00016}
}

@inproceedings{roumpani_creating_2013,
  title = {Creating, Visualizing and Modelling the Realtime City},
  booktitle = {Proceedings of {{Hybrid City II}} ‚Äò{{Subtle rEvolutions}}‚Äô {{Conference}}},
  date = {2013},
  keywords = {City dashboard,CASA},
  author = {Roumpani, F. and O‚ÄôBrien, O. and Hudson-Smith, A.},
  file = {/home/robin/Dropbox/Biblio/Roumpani_O‚ÄôBrienet-al_2013_Creating,_visualizing_and_modelling_the_realtime_city.pdf},
  note = {00002}
}

@article{kitchin_knowing_2015,
  title = {Knowing and Governing Cities through Urban Indicators, City Benchmarking and Real-Time Dashboards},
  volume = {2},
  issn = {null},
  url = {https://doi.org/10.1080/21681376.2014.983149},
  doi = {10.1080/21681376.2014.983149},
  abstract = {Since the mid-1990s a plethora of indicator projects have been developed and adopted by cities seeking to measure and monitor various aspects of urban systems. These have been accompanied by city benchmarking endeavours that seek to compare intra- and inter-urban performance. More recently, the data underpinning such projects have started to become more open to citizens, more real-time in nature generated through sensors and locative/social media, and displayed via interactive visualisations and dashboards that can be accessed via the internet. In this paper, we examine such initiatives arguing that they advance a narrowly conceived but powerful realist epistemology ‚Äì the city as visualised facts ‚Äì that is reshaping how managers and citizens come to know and govern cities. We set out how and to what ends indicator, benchmarking and dashboard initiatives are being employed by cities. We argue that whilst these initiatives often seek to make urban processes and performance more transparent and to improve decision making, they are also underpinned by a naive instrumental rationality, are open to manipulation by vested interests, and suffer from often unacknowledged methodological and technical issues. Drawing on our own experience of working on indicator and dashboard projects, we argue for a conceptual re-imaging of such projects as data assemblages ‚Äì complex, politically-infused, socio-technical systems that, rather than reflecting cities, actively frame and produce them.},
  number = {1},
  journaltitle = {Regional Studies, Regional Science},
  urldate = {2018-06-29},
  date = {2015-01-01},
  pages = {6-28},
  keywords = {cities,City dashboard,benchmarking,dashboards,data assemblage,epistemology,governance,indicators,real-time},
  author = {Kitchin, Rob and Lauriault, Tracey P. and McArdle, Gavin},
  file = {/home/robin/Dropbox/Biblio/Kitchin_Lauriaultet-al_2015_Knowing_and_governing_cities_through_urban_indicators,_city_benchmarking_and.pdf;/home/robin/.mozilla/firefox/2czgwusc.default/zotero/storage/RFS3YZFQ/21681376.2014.html},
  note = {00169}
}

@article{batty_perspective_2015,
  title = {A Perspective on City Dashboards},
  volume = {2},
  issn = {null},
  url = {https://doi.org/10.1080/21681376.2014.987540},
  doi = {10.1080/21681376.2014.987540},
  number = {1},
  journaltitle = {Regional Studies, Regional Science},
  urldate = {2018-06-29},
  date = {2015-01-01},
  pages = {29-32},
  keywords = {city dashboard},
  author = {Batty, Michael},
  file = {/home/robin/Dropbox/Biblio/Batty_2015_A_perspective_on_city_dashboards.pdf;/home/robin/.mozilla/firefox/2czgwusc.default/zotero/storage/T88TXKJT/21681376.2014.html},
  note = {00016}
}

@book{few_information_2006,
  title = {Information {{Dashboard Design}}: {{The Effective Visual Communication}} of {{Data}}},
  isbn = {978-0-596-10016-2},
  shorttitle = {Information {{Dashboard Design}}},
  abstract = {Dashboards have become popular in recent years as uniquely powerful tools for communicating important information at a glance. Although dashboards are potentially powerful, this potential is rarely realized. The greatest display technology in the world won't solve this if you fail to use effective visual design. And if a dashboard fails to tell you precisely what you need to know in an instant, you'll never use it, even if it's filled with cute gauges, meters, and traffic lights. Don't let your investment in dashboard technology go to waste.This book will teach you the visual design skills you need to create dashboards that communicate clearly, rapidly, and compellingly. Information Dashboard Design will explain how to:Avoid the thirteen mistakes common to dashboard designProvide viewers with the information they need quickly and clearlyApply what we now know about visual perception to the visual presentation of informationMinimize distractions, cliches, and unnecessary embellishments that create confusionOrganize business information to support meaning and usabilityCreate an aesthetically pleasing viewing experienceMaintain consistency of design to provide accurate interpretationOptimize the power of dashboard technology by pairing it with visual effectivenessStephen Few has over 20 years of experience as an IT innovator, consultant, and educator. As Principal of the consultancy Perceptual Edge, Stephen focuses on data visualization for analyzing and communicating quantitative business information. He provides consulting and training services, speaks frequently at conferences, and teaches in the MBA program at the University of California in Berkeley. He is also the author of Show Me the Numbers: Designing Tables and Graphs to Enlighten. Visit his website at www.perceptualedge.com.},
  publisher = {{O'Reilly Media, Inc.}},
  date = {2006},
  author = {Few, Stephen},
  note = {00001}
}

@article{rivard_are_2004,
  title = {Are {{You Drowning}} in {{BI Reports}}? {{Using Analytical Dashboards}} to Cut through the Clutter},
  journaltitle = {DM Review, http://goo. gl/hle9Wc},
  date = {2004},
  author = {Rivard, Kurt and Cogswell, Doug},
  file = {/home/robin/Dropbox/Biblio/Rivard_Cogswell_2004_Are_You_Drowning_in_BI_Reports.pdf},
  note = {00000}
}

@book{iannone_flexdashboard_2018,
  title = {Flexdashboard: {{R Markdown Format}} for {{Flexible Dashboards}}},
  url = {https://CRAN.R-project.org/package=flexdashboard},
  date = {2018},
  author = {Iannone, Richard and Allaire, JJ and Borges, Barbara},
  note = {00000 
R package version 0.5.1.1}
}

@inproceedings{heinrich_state_2013,
  langid = {english},
  title = {State of the {{Art}} of {{Parallel Coordinates}}},
  url = {https://diglib.eg.org:443/handle/10.2312/conf.EG2013.stars.095-116},
  doi = {http://dx.doi.org/10.2312/conf/EG2013/stars/095-116},
  abstract = {This work presents a survey of the current state of the art of visualization techniques for parallel coordinates. It covers geometric models for constructing parallel coordinates and reviews methods for creating and understanding visual representations of parallel coordinates. The classification of these methods is based on a taxonomy that was established from the literature and is aimed at guiding researchers to find existing techniques and identifying white spots that require further research. The techniques covered in this survey are further related to an established taxonomy of knowledge-discovery tasks to support users of parallel coordinates in choosing a technique for their problem at hand. Finally, we discuss the challenges in constructing and understanding parallel-coordinates plots and provide some examples from different application domains.},
  eventtitle = {Eurographics ({{STARs}})},
  urldate = {2018-08-02},
  date = {2013},
  pages = {95--116},
  keywords = {Visual analytics,Parallel coordinates},
  author = {Heinrich, Julian and Weiskopf, Daniel},
  file = {/home/robin/Dropbox/Biblio/Heinrich_Weiskopf_2013_State_of_the_Art_of_Parallel_Coordinates.pdf;/home/robin/.mozilla/firefox/2czgwusc.default/zotero/storage/L7YZC6LZ/conf.EG2013.stars.html}
}

@article{few_multivariate_2006,
  title = {Multivariate Analysis Using Parallel Coordinates},
  journaltitle = {Perceptual edge},
  date = {2006},
  pages = {1--9},
  author = {Few, Stephen},
  file = {/home/robin/Dropbox/Biblio/Few_2006_Multivariate_analysis_using_parallel_coordinates.pdf},
  note = {00039}
}

@incollection{inselberg_parallel_1987,
  title = {Parallel Coordinates for Visualizing Multi-Dimensional Geometry},
  booktitle = {Computer {{Graphics}} 1987},
  publisher = {{Springer}},
  date = {1987},
  pages = {25--44},
  author = {Inselberg, Alfred and Dimsdale, Bernard},
  file = {/home/robin/Dropbox/Biblio/Inselberg_Dimsdale_1987_Parallel_coordinates_for_visualizing_multi-dimensional_geometry.pdf;/home/robin/.mozilla/firefox/2czgwusc.default/zotero/storage/UR563QCZ/978-4-431-68057-4_3.html},
  note = {01538}
}

@unpublished{cura_making_2017,
  venue = {{York, United-Kingdom}},
  title = {Making Large Spatio-Temporal Data Analysis Easier. {{Illustrated}} Plea for Using (Geo){{Visual Analytics}}.},
  url = {http://www.geog.leeds.ac.uk/ectqg17/home.html},
  abstract = {In 1998, Peter Fisher advocated for an improvement of geographic data exploration, considering that ‚Äúsoftware tools need to be developed to transform data into intelligible views and exploratory tools. [...] It must be made much easier‚Äù (Fisher, 1998). For Andrienko et al. (2014), this issue ‚Äúremains sharply pertinent nearly 20 years later‚Äù.
Fact is, despite being able to rely on some very useful and praised tools like GeoDa, most of the geographical data is still explored and analyzed within the bounds of traditional GIS software, especially within the Theoretical and Quantitative Geography community.
Alan MacEachren (2017) called for a renewed use of (Geo)Visual Analytics. He especially insists on the opportunities that Big Data offers, pleading ‚Äúfor a ‚Äòhuman-in-the-loop‚Äô (geo)Visual Analytics approach to leveraging big (geo)data‚Äù.
I suggest that this approach can also be applied to regular spatio-temporal large datasets. Together with the emergence of the ‚Äúgeodata science‚Äù and the growing complexity of analysis methods, most of the recent research on this field focus on developing new complex methods. With today‚Äôs computation power increase and more employable technologies, TQG community could make use of the simplest visual exploration methods that could not be applied on large dataset until recently. Among these, I consider traditional methods such as small multiples, linked views and filters, temporal as well as spatial and thematic.
In this talk, I‚Äôll present such a use on three large and inherently spatial and temporal datasets. (1) The first comes from the Google Location History of an anonymous user. It consists of a frequent recording (every 3 minutes) of the user‚Äôs smartphone location. Albeit the lack of contextual data, we will show how this mass of data can allow to gather very personal informations about the user. (2) We also present an exploration of the CitiBike dataset. Originating from New-York City bike sharing system, it consists of a record for each trip. Together with spatio-temporal informations, thematic data like age and gender of users can help figure out different uses of the bike system through time and space. (3) The last dataset is about transit : for multiple road segments, road-censors in Paris registers the car traffic every hour. Gathered over a year, this very large dataset can help detecting patterns and special events in the car flow.
Those datasets and questions were addressed by creating specific web applications dedicated to their visual exploration, and I will try to expose the generality of such an approach. A particular attention will be dedicated to showing how easy it can be to build such ad-hoc exploratory tools, and, more importantly, how simple it can be to use.},
  eventtitle = {20th {{European Colloquium}} on {{Theoretical}} and {{Quantitative Geography}} ({{ECTQG}} 2017)},
  date = {2017-09-11},
  author = {Cura, Robin},
  file = {/home/robin/.mozilla/firefox/2czgwusc.default/zotero/storage/KNM6IDPD/CURA_GeoVA_ECTQG2017.pptx},
  note = {00000 
cites: cura\_making\_2017}
}

@article{hegselmann_thomas_2017,
  title = {Thomas {{C}}. {{Schelling}} and {{James M}}. {{Sakoda}}: {{The Intellectual}}, {{Technical}}, and {{Social History}} of a {{Model}}},
  volume = {20},
  issn = {1460-7425},
  shorttitle = {Thomas {{C}}. {{Schelling}} and {{James M}}. {{Sakoda}}},
  number = {3},
  journaltitle = {Journal of Artificial Societies and Social Simulation},
  shortjournal = {JASSS},
  date = {2017},
  pages = {15},
  author = {Hegselmann, Rainer},
  file = {/home/robin/.mozilla/firefox/2czgwusc.default/zotero/storage/IZ9TBKPZ/15.html},
  note = {00000}
}

@misc{pafka_benchm-databases_2017,
  langid = {english},
  title = {Benchm-Databases: {{A}} Minimal Benchmark of Various Tools (Statistical Software, Databases Etc.) for Working with Tabular Data of Moderately Large Sizes (Interactive Data Analysis)},
  url = {https://github.com/szilard/benchm-databases},
  shorttitle = {Benchm-Databases},
  urldate = {2018-08-25},
  date = {2017-07-25},
  author = {Pafka, Szilard},
  origdate = {2015-02-25T04:56:02Z},
  note = {00000}
}

@article{liu_effects_2014,
  title = {The {{Effects}} of {{Interactive Latency}} on {{Exploratory Visual Analysis}}},
  volume = {20},
  issn = {1077-2626},
  doi = {10.1109/TVCG.2014.2346452},
  abstract = {To support effective exploration, it is often stated that interactive visualizations should provide rapid response times. However, the effects of interactive latency on the process and outcomes of exploratory visual analysis have not been systematically studied. We present an experiment measuring user behavior and knowledge discovery with interactive visualizations under varying latency conditions. We observe that an additional delay of 500ms incurs significant costs, decreasing user activity and data set coverage. Analyzing verbal data from think-aloud protocols, we find that increased latency reduces the rate at which users make observations, draw generalizations and generate hypotheses. Moreover, we note interaction effects in which initial exposure to higher latencies leads to subsequently reduced performance in a low-latency setting. Overall, increased latency causes users to shift exploration strategy, in turn affecting performance. We discuss how these results can inform the design of interactive analysis tools.},
  number = {12},
  journaltitle = {IEEE Transactions on Visualization and Computer Graphics},
  date = {2014-12},
  pages = {2122-2131},
  keywords = {Cognition,Computer Graphics,data analysis,data mining,Data visualization,Databases; Factual,delay,exploratory analysis,exploratory visual analysis,Humans,Image color analysis,Informatics,Interaction,interactive analysis tools,interactive latency effects,Interactive services,interactive systems,interactive visualization,knowledge discovery,latency,scalability,Task Performance and Analysis,think-aloud protocols,Time Factors,user behavior,user performance,verbal analysis,verbal data analysis,Visual Analysis,Visual analytics,Visualization},
  author = {Liu, Z. and Heer, J.},
  file = {/home/robin/Dropbox/Biblio/Liu_Heer_2014_The_Effects_of_Interactive_Latency_on_Exploratory_Visual_Analysis.pdf;/home/robin/.mozilla/firefox/2czgwusc.default/zotero/storage/G2QH3VI8/6876022.html},
  note = {00125}
}

@inproceedings{zaamoune_new_2013,
  title = {A New Relational Spatial {{OLAP}} Approach for Multi-Resolution and Spatio-Multidimensional Analysis of Incomplete Field Data},
  booktitle = {{{ICEIS}} 2013 {{INSTICC International Conference}} on {{Enterprise Information Systems}}},
  date = {2013},
  pages = {p},
  keywords = {OLAP,SOLAP},
  author = {Zaamoune, Mehdi and Bimonte, Sandro and Pinet, Fran√ßois and Beaune, Philippe},
  file = {/home/robin/Dropbox/Biblio/Zaamoune_Bimonteet-al_2013_A_new_relational_spatial_OLAP_approach_for_multi-resolution_and.pdf},
  note = {00010}
}

@inproceedings{bimonte_towards_2005,
  location = {{New York, NY, USA}},
  title = {Towards a {{Spatial Multidimensional Model}}},
  isbn = {978-1-59593-162-7},
  url = {http://doi.acm.org/10.1145/1097002.1097009},
  doi = {10.1145/1097002.1097009},
  abstract = {Data warehouses and OLAP systems help to interactively analyze huge volume of data. This data, extracted from transactional databases, frequently contains spatial information which is useful for decision-making process. Integration of spatial data in multidimensional models leads to the concept of SOLAP (Spatial OLAP). Using a spatial measure as a geographical object, i.e. with geometric and descriptive attributes, raises problems regarding the aggregation operation in its semantic and implementation aspects. This paper shows the requirements for a multidimensional spatial data model and presents a multidimensional data model which is able to support complex objects as measures, inter-dependent attributes for measures and aggregation functions, use of ad-hoc aggregation functions and n to n relations between fact and dimension, in order to handle geographical data, according to its particular nature in an OLAP context.},
  booktitle = {Proceedings of the 8th {{ACM International Workshop}} on {{Data Warehousing}} and {{OLAP}}},
  series = {DOLAP '05},
  publisher = {{ACM}},
  urldate = {2018-08-27},
  date = {2005},
  pages = {39--46},
  keywords = {data modelling,multidimensional data,spatial data,spatial OLAP},
  author = {Bimonte, S. and Tchounikine, A. and Miquel, M.},
  file = {/home/robin/Dropbox/Biblio/Bimonte_Tchounikineet-al_2005_Towards_a_Spatial_Multidimensional_Model.pdf},
  note = {00075}
}

@book{bimonte_integration_2007,
  title = {Int√©gration de l'information G√©ographique Dans Les Entrep√¥ts de Donn√©es et l'analyse En Ligne : De La Mod√©lisation √† La Visualisation},
  url = {http://www.theses.fr/2007ISAL0105},
  shorttitle = {Int√©gration de l'information G√©ographique Dans Les Entrep√¥ts de Donn√©es et l'analyse En Ligne},
  abstract = {Les syst√®mes d‚Äôentrep√¥ts de donn√©es et OLAP sont de solutions pour l‚Äôanalyse d√©cisionnelle. L‚Äôint√©gration des donn√©es spatiales dans l‚Äô OLAP est un enjeu majeur. L‚Äôinformation g√©ographique est tr√®s fr√©quemment pr√©sente dans les donn√©es, mais g√©n√©ralement sous-employ√©e dans le processus d√©cisionnel. Le couplage de syst√®mes OLAP et de Syst√®mes d‚ÄôInformations G√©ographiques au sein de syst√®mes OLAP Spatial (SOLAP) est une voie prometteuse. La majorit√© des solutions SOLAP r√©duisent l‚Äôinformation g√©ographique √† la seule composante spatiale, limitant ainsi les capacit√©s d‚Äôanalyse du paradigme spatio-multidimensionnel. Nous proposons un mod√®le formel (GeoCube) et une alg√®bre associ√©e, qui reformule les concepts du SOLAP afin d‚Äôintroduire les aspects s√©mantiques et spatiaux de l‚Äôinformation g√©ographique dans l‚Äôanalyse multidimensionnelle. Cela se traduit par une mod√©lisation des mesures sous forme d'objets g√©ographiques, dans une vision compl√®tement sym√©trique entre mesures et dimensions. Ainsi une mesure peut participer √† une hi√©rarchie. Nous proposons une alg√®bre qui fournit les op√©rateurs de forage et de coupe, un op√©rateur qui permet d'intervertir mesure et dimension et des op√©rateurs de navigation au sein de la hi√©rarchie de mesures. Cette alg√®bre, gr√¢ce aux op√©rateurs qui modifient dynamiquement la structure de l‚Äôhypercube, permet de concilier analyse OLAP et analyse spatiale. Nous avons r√©alis√© un prototype web conforme √† GeoCube. Pour d√©crire nos solutions, nous utilisons des donn√©es environnementales de la lagune de Venise. Enfin, nous proposons un nouveau paradigme de visualisation et d‚Äôinteraction pour l‚Äôanalyse des mesures g√©ographiques.},
  publisher = {{Lyon, INSA}},
  urldate = {2018-08-27},
  date = {2007-01-01},
  author = {Bimonte, Sandro},
  file = {/home/robin/Dropbox/Biblio/Bimonte_2007_Int√©gration_de_l'information_g√©ographique_dans_les_entrep√¥ts_de_donn√©es_et.pdf;/home/robin/.mozilla/firefox/2czgwusc.default/zotero/storage/NSC73M5E/2007ISAL0105.html},
  note = {00029}
}

@inproceedings{agarwal_blinkdb_2013,
  location = {{New York, NY, USA}},
  title = {{{BlinkDB}}: {{Queries}} with {{Bounded Errors}} and {{Bounded Response Times}} on {{Very Large Data}}},
  isbn = {978-1-4503-1994-2},
  url = {http://doi.acm.org/10.1145/2465351.2465355},
  doi = {10.1145/2465351.2465355},
  shorttitle = {{{BlinkDB}}},
  abstract = {In this paper, we present BlinkDB, a massively parallel, approximate query engine for running interactive SQL queries on large volumes of data. BlinkDB allows users to trade-off query accuracy for response time, enabling interactive queries over massive data by running queries on data samples and presenting results annotated with meaningful error bars. To achieve this, BlinkDB uses two key ideas: (1) an adaptive optimization framework that builds and maintains a set of multi-dimensional stratified samples from original data over time, and (2) a dynamic sample selection strategy that selects an appropriately sized sample based on a query's accuracy or response time requirements. We evaluate BlinkDB against the well-known TPC-H benchmarks and a real-world analytic workload derived from Conviva Inc., a company that manages video distribution over the Internet. Our experiments on a 100 node cluster show that BlinkDB can answer queries on up to 17 TBs of data in less than 2 seconds (over 200 x faster than Hive), within an error of 2-10\%.},
  booktitle = {Proceedings of the 8th {{ACM European Conference}} on {{Computer Systems}}},
  series = {EuroSys '13},
  publisher = {{ACM}},
  urldate = {2018-08-31},
  date = {2013},
  pages = {29--42},
  keywords = {database,time limit,estimation,query approximation},
  author = {Agarwal, Sameer and Mozafari, Barzan and Panda, Aurojit and Milner, Henry and Madden, Samuel and Stoica, Ion},
  file = {/home/robin/Dropbox/Biblio/Agarwal_Mozafariet-al_2013_BlinkDB.pdf},
  note = {00474}
}

@inproceedings{zeng_iolap_2016,
  location = {{New York, NY, USA}},
  title = {{{iOLAP}}: {{Managing Uncertainty}} for {{Efficient Incremental OLAP}}},
  isbn = {978-1-4503-3531-7},
  url = {http://doi.acm.org/10.1145/2882903.2915240},
  doi = {10.1145/2882903.2915240},
  shorttitle = {{{iOLAP}}},
  abstract = {The size of data and the complexity of analytics continue to grow along with the need for timely and cost-effective analysis. However, the growth of computation power cannot keep up with the growth of data. This calls for a paradigm shift from traditional batch OLAP processing model to an incremental OLAP processing model. In this paper, we propose iOLAP, an incremental OLAP query engine that provides a smooth trade-off between query accuracy and latency, and fulfills a full spectrum of user requirements from approximate but timely query execution to a more traditional accurate query execution. iOLAP enables interactive incremental query processing using a novel mini-batch execution model---given an OLAP query, iOLAP first randomly partitions the input dataset into smaller sets (mini-batches) and then incrementally processes through these mini-batches by executing a delta update query on each mini-batch, where each subsequent delta update query computes an update based on the output of the previous one. The key idea behind iOLAP is a novel delta update algorithm that models delta processing as an uncertainty propagation problem, and minimizes the recomputation during each subsequent delta update by minimizing the uncertainties in the partial (including intermediate) query results. We implement iOLAP on top of Apache Spark and have successfully demonstrated it at scale on over 100 machines. Extensive experiments on a multitude of queries and datasets demonstrate that iOLAP can deliver approximate query answers for complex OLAP queries orders of magnitude faster than traditional OLAP engines, while continuously delivering updates every few seconds.},
  booktitle = {Proceedings of the 2016 {{International Conference}} on {{Management}} of {{Data}}},
  series = {SIGMOD '16},
  publisher = {{ACM}},
  urldate = {2018-08-31},
  date = {2016},
  pages = {1347--1361},
  keywords = {OLAP,approximate query processing,bootstrap,incremental},
  author = {Zeng, Kai and Agarwal, Sameer and Stoica, Ion},
  file = {/home/robin/Dropbox/Biblio/Zeng_Agarwalet-al_2016_iOLAP.pdf},
  note = {00012}
}

@online{patel_speed_2011,
  langid = {english},
  title = {Speed {{Is A Killer}}},
  url = {https://neilpatel.com/blog/speed-is-a-killer/},
  shorttitle = {Speed {{Is A Killer}}},
  abstract = {Why Decreasing Page Load Time Can Drastically Increase Conversions

Can the speed of your website really have that much of an effect on your sales? Even if your site isn‚Äôt loading too slowly, can it still be improved? And how does Google factor into all of this? You might be surprised.},
  journaltitle = {Neil Patel},
  urldate = {2018-09-02},
  date = {2011-05-10},
  author = {Patel, Neil},
  file = {/home/robin/.mozilla/firefox/2czgwusc.default/zotero/storage/F5TN8K89/speed-is-a-killer.html},
  note = {00000}
}

@online{elliott_how_2017,
  title = {How {{Page Load Time Affects Bounce Rate}} and {{Page Views}}},
  url = {https://www.section.io/blog/page-load-time-bounce-rate/},
  abstract = {How page speed affects bounce rate and number of pages viewed per session. Website performance impact on revenue.},
  urldate = {2018-09-02},
  date = {2017-08-10},
  author = {Elliott, Roxana},
  editora = {{Section.io}},
  editoratype = {collaborator},
  file = {/home/robin/.mozilla/firefox/2czgwusc.default/zotero/storage/2DG3LJAC/page-load-time-bounce-rate.html},
  note = {00000}
}

@inproceedings{root_mapd_2016,
  location = {{New York, NY, USA}},
  title = {{{MapD}}: {{A GPU}}-Powered {{Big Data Analytics}} and {{Visualization Platform}}},
  isbn = {978-1-4503-4282-7},
  url = {http://doi.acm.org/10.1145/2897839.2927468},
  doi = {10.1145/2897839.2927468},
  shorttitle = {{{MapD}}},
  abstract = {MapD, or "Massively Parallel Database", is a big data analytics platform that can query and visualize big data up to 100x faster than other systems. It leverages the massive parallelism of commodity GPUs to execute SQL queries over multi-billion row datasets with millisecond response times, and optionally render the results using the GPU's native graphics pipeline. Depending on the use case, MapD can be used as a standalone SQL database or as a data visualization suite by using its own visualization frontend (see Fig. 1) or by integrating it with other third-party toolkits.},
  booktitle = {{{ACM SIGGRAPH}} 2016 {{Talks}}},
  series = {SIGGRAPH '16},
  publisher = {{ACM}},
  urldate = {2018-09-02},
  date = {2016},
  pages = {73:1--73:2},
  keywords = {data visualization,database,analytics,CUDA,GPU,OpenGL},
  author = {Root, Christopher and Mostak, Todd},
  file = {/home/robin/Dropbox/Biblio/Root_Mostak_2016_MapD.pdf}
}

@inproceedings{vermeij_monetdb_2008,
  title = {{{MonetDB}}, a Novel Spatial Columnstore Dbms},
  booktitle = {Academic {{Proceedings}} of the 2008 {{Free}} and {{Open Source}} for {{Geospatial}} ({{FOSS4G}}) {{Conference}}, {{OSGeo}}},
  date = {2008},
  pages = {193--199},
  author = {Vermeij, Maarten and Quak, Wilko and Kersten, Martin and Nes, Niels},
  file = {/home/robin/Dropbox/Biblio/Vermeij_Quaket-al_2008_Monetdb,_a_novel_spatial_columnstore_dbms.pdf},
  note = {00015}
}

@inproceedings{raasveldt_monetdblite_2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1805.08520},
  location = {{New York, NY, USA}},
  title = {{{MonetDBLite}}: {{An Embedded Analytical Database}}},
  isbn = {978-1-4503-4703-7},
  url = {http://arxiv.org/abs/1805.08520},
  doi = {10.1145/3183713.3183722},
  shorttitle = {{{MonetDBLite}}},
  abstract = {While traditional RDBMSes offer a lot of advantages, they require significant effort to setup and to use. Because of these challenges, many data scientists and analysts have switched to using alternative data management solutions. These alternatives, however, lack features that are standard for RDBMSes, e.g. out-of-core query execution. In this paper, we introduce the embedded analytical database MonetDBLite. MonetDBLite is designed to be both highly efficient and easy to use in conjunction with standard analytical tools. It can be installed using standard package managers, and requires no configuration or server management. It is designed for OLAP scenarios, and offers near-instantaneous data transfer between the database and analytical tools, all the while maintaining the transactional guarantees and ACID properties of a standard relational system. These properties make MonetDBLite highly suitable as a storage engine for data used in analytics, machine learning and classification tasks.},
  booktitle = {Proceedings of the 2018 {{International Conference}} on {{Management}} of {{Data}}},
  series = {SIGMOD '18},
  publisher = {{ACM}},
  urldate = {2018-09-02},
  date = {2018},
  pages = {1837--1838},
  keywords = {machine learning,embedded databases,olap},
  author = {Raasveldt, Mark and M√ºhleisen, Hannes},
  file = {/home/robin/Dropbox/Biblio/Raasveldt_2018_MonetDBLite.pdf},
  note = {00001}
}

@inreference{noauthor_star_2018,
  langid = {english},
  title = {Star Schema},
  url = {https://en.wikipedia.org/w/index.php?title=Star_schema&oldid=853084099},
  abstract = {In computing, the star schema is the simplest style of data mart schema and is the approach most widely used to develop data warehouses and dimensional data marts. The star schema consists of one or more fact tables referencing any number of dimension tables. The star schema is an important special case of the snowflake schema, and is more effective for handling simpler queries.The star schema gets its name from the physical model's resemblance to a star shape with a fact table at its center and the dimension tables surrounding it representing the star's points.},
  booktitle = {Wikipedia},
  urldate = {2018-09-03},
  date = {2018-08-02},
  file = {/home/robin/.mozilla/firefox/2czgwusc.default/zotero/storage/MENE5EAY/index.html},
  note = {00001 
Page Version ID: 853084099}
}

@inreference{noauthor_snowflake_2018,
  langid = {english},
  title = {Snowflake Schema},
  url = {https://en.wikipedia.org/w/index.php?title=Snowflake_schema&oldid=856363739},
  abstract = {In computing, a snowflake schema is a logical arrangement of tables in a multidimensional database such that the entity relationship diagram resembles a snowflake  shape. The snowflake schema is represented by centralized fact tables which are connected to multiple dimensions..  "Snowflaking" is a method of normalizing the dimension tables in a star schema. When it is completely normalized along all the dimension tables, the resultant structure resembles a snowflake with the fact table in the middle. The principle behind snowflaking is normalization of the dimension tables by removing low cardinality attributes and forming separate tables.The snowflake schema is similar to the star schema. However, in the snowflake schema, dimensions are normalized into multiple related tables, whereas the star schema's dimensions are denormalized with each dimension represented by a single table. A complex snowflake shape emerges when the dimensions of a snowflake schema are elaborate, having multiple levels of relationships, and the child tables have multiple parent tables ("forks in the road").},
  booktitle = {Wikipedia},
  urldate = {2018-09-03},
  date = {2018-08-24},
  file = {/home/robin/.mozilla/firefox/2czgwusc.default/zotero/storage/B5UE3R46/index.html},
  note = {00001 
Page Version ID: 856363739}
}

@book{keim_mastering_2010,
  title = {Mastering the {{Information Age Solving Problems}} with {{Visual Analytics}}},
  publisher = {{Eurographics Association}},
  date = {2010},
  author = {Keim, Daniel and Kohlhammer, J√∂rn and Ellis, Geoffrey and Mansmann, Florian},
  file = {/home/robin/.mozilla/firefox/2czgwusc.default/zotero/storage/UUBQD5QQ/Capture d‚Äô√©cran de 2018-09-05 14-44-15.png;/home/robin/Dropbox/Biblio/Ellis_Mansmann_2010_Mastering_the_information_age_solving_problems_with_visual_analytics.pdf;/home/robin/Dropbox/Biblio/Keim_Kohlhammeret-al_2010_Mastering_the_Information_Age_Solving_Problems_with_Visual_Analytics.pdf;/home/robin/.mozilla/firefox/2czgwusc.default/zotero/storage/E9S7BDK5/14803.html},
  note = {00019}
}

@article{dos_santos_gaining_2004,
  title = {Gaining Understanding of Multivariate and Multidimensional Data through Visualization},
  volume = {28},
  issn = {0097-8493},
  url = {http://www.sciencedirect.com/science/article/pii/S0097849304000251},
  doi = {10.1016/j.cag.2004.03.013},
  abstract = {High dimensionality is a major challenge for data visualization. Parameter optimization problems require an understanding of the behaviour of the objective function in the n-dimensional space around the optimum‚Äîthis is multidimensional visualization and is the traditional domain of scientific visualization. Large data tables require us to understand the relationship between attributes in the table‚Äîthis is multivariate visualization and is an important aspect of information visualization. Common to both types of ‚Äòhigh-dimensional‚Äô visualization is a need to reduce the dimensionality for display. In this paper we present a uniform approach to the filtering of both multidimensional and multivariate data, to allow extraction of data subject to constraints on their position or value within an n-dimensional window, and on choice of dimensions for display. A simple example of understanding the trajectory of solutions from an optimization algorithm is given‚Äîthis involves a combination of multidimensional and multivariate data.},
  number = {3},
  journaltitle = {Computers \& Graphics},
  shortjournal = {Computers \& Graphics},
  urldate = {2018-09-05},
  date = {2004-06-01},
  pages = {311-325},
  keywords = {Visualization,Multidimensional,Multivariate,Reference model},
  author = {dos Santos, Selan and Brodlie, Ken},
  options = {useprefix=true},
  file = {/home/robin/Dropbox/Biblio/dos_Santos_Brodlie_2004_Gaining_understanding_of_multivariate_and_multidimensional_data_through.pdf;/home/robin/.mozilla/firefox/2czgwusc.default/zotero/storage/FRV9DUQI/S0097849304000251.html},
  note = {00107}
}

@incollection{fekete_infrastructure_2010,
  title = {Infrastructure},
  url = {https://hal.inria.fr/hal-00696814},
  abstract = {Supporting the strong demand in data storage, computation and interactive performances required by visual analytics applications is still a challenge. All currently existing visual analytics applications need to build their own specialised infrastructure for their speciÔ¨Åc problem. One of the most difficult issues of visual analytics is that it is both user driven and data-driven. It is user-driven because during the interactive steps of the analysis, the user is specifying algorithms and parameters to explore the data. It is also data-driven because new data is made available to the user at unpredictable times, such as when algorithms run or databases are updated. This chapter describes the landscape of software and hardware solutions to address this difficult issues.},
  booktitle = {Mastering the Information Age - {{Solving}} Problems with Visual Analytics},
  publisher = {{Eurographics Association}},
  urldate = {2018-09-05},
  date = {2010-11},
  pages = {87-108},
  author = {Fekete, Jean-Daniel},
  editor = {Association, Eurographics},
  file = {/home/robin/Dropbox/Biblio/Fekete_2010_Infrastructure.pdf},
  note = {00178}
}

@article{coltekin_geovisualization_2018,
  title = {Geovisualization},
  volume = {2018},
  issn = {25772848},
  url = {https://gistbok.ucgis.org/bok-topics/geovisualization},
  doi = {10.22224/gistbok/2018.2.6},
  number = {Q2},
  journaltitle = {Geographic Information Science \& Technology Body of Knowledge},
  urldate = {2018-09-06},
  date = {2018-04-01},
  author = {√á√∂ltekin, Arzu and Janetzko, Halld√≥r and Fabrikant, Sara},
  note = {00009}
}

@article{maceachren_geovisualization_2004,
  title = {Geovisualization for {{Knowledge Construction}} and {{Decision Support}}},
  volume = {24},
  issn = {0272-1716},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3181162/},
  number = {1},
  journaltitle = {IEEE computer graphics and applications},
  shortjournal = {IEEE Comput Graph Appl},
  urldate = {2018-09-06},
  date = {2004},
  pages = {13-17},
  author = {MacEachren, Alan M. and Gahegan, Mark and Pike, William and Brewer, Isaac and Cai, Guoray and Lengerich, Eugene and Hardisty, Frank},
  file = {/home/robin/Dropbox/Biblio/MacEachren_Gaheganet-al_2004_Geovisualization_for_Knowledge_Construction_and_Decision_Support.pdf},
  eprinttype = {pmid},
  eprint = {15384662},
  pmcid = {PMC3181162},
  note = {00256}
}

@article{fotheringham_trends_1999,
  langid = {english},
  title = {Trends in Quantitative Methods {{III}}: Stressing the Visual},
  volume = {23},
  issn = {0309-1325},
  url = {https://doi.org/10.1191/030913299667756016},
  doi = {10.1191/030913299667756016},
  shorttitle = {Trends in Quantitative Methods {{III}}},
  number = {4},
  journaltitle = {Progress in Human Geography},
  shortjournal = {Progress in Human Geography},
  urldate = {2018-09-07},
  date = {1999-12-01},
  pages = {597-606},
  keywords = {GWR,parallel coordinates,spatial visualisation},
  author = {Fotheringham, A. Stewart},
  file = {/home/robin/Dropbox/Biblio/Fotheringham_1999_Trends_in_quantitative_methods_III.pdf},
  note = {00038}
}

@inproceedings{mackenzie_lag_1993,
  location = {{New York, NY, USA}},
  title = {Lag {{As}} a {{Determinant}} of {{Human Performance}} in {{Interactive Systems}}},
  isbn = {978-0-89791-575-5},
  url = {http://doi.acm.org/10.1145/169059.169431},
  doi = {10.1145/169059.169431},
  abstract = {The sources of lag (the delay between input action and output response) and its effects on human performance are discussed. We measured the effects in a study of target acquisition using the classic Fitts' law paradigm with the addition of four lag conditions. At the highest lag tested (225 ms), movement times and error rates increased by 64\% and 214\% respectively, compared to the zero lag condition. We propose a model according to which lag should have a multiplicative effect on Fitts' index of difficulty. The model accounts for 94\% of the variance and is better than alternative models which propose only an additive effect for lag. The implications for the design of virtual reality systems are discussed.},
  booktitle = {Proceedings of the {{INTERACT}} '93 and {{CHI}} '93 {{Conference}} on {{Human Factors}} in {{Computing Systems}}},
  series = {CHI '93},
  publisher = {{ACM}},
  urldate = {2018-09-10},
  date = {1993},
  pages = {488--493},
  keywords = {feedback delay,Fitts' law,human performance modeling,lag,speed-accuracy tradeoff,virtual reality},
  author = {MacKenzie, I. Scott and Ware, Colin},
  file = {/home/robin/Dropbox/Biblio/MacKenzie_Ware_1993_Lag_As_a_Determinant_of_Human_Performance_in_Interactive_Systems.pdf},
  note = {00442}
}

@inproceedings{forch_are_2017,
  langid = {english},
  title = {Are 100 Ms {{Fast Enough}}? {{Characterizing Latency Perception Thresholds}} in {{Mouse}}-{{Based Interaction}}},
  isbn = {978-3-319-58475-1},
  shorttitle = {Are 100 Ms {{Fast Enough}}?},
  abstract = {The claim that 100 ms system latency is fast enough for an optimal interaction with highly interactive computer systems has been challenged by several studies demonstrating that users are able to perceive latencies well below the 100 ms mark. Although a high amount of daily computer interactions is still characterized by mouse-based interaction, to date only few studies about latency perception thresholds have employed a corresponding interaction paradigm. Therefore, we determined latency perception thresholds in a mouse-based computer interaction task. We also tested whether user characteristics, such as experience with latency in computer interaction and interaction styles, might be related to inter-individual differences in latency perception thresholds, as results of previous studies indicate that there is considerable inter-individual variance in latency perception thresholds. Our results show that latency perception thresholds for a simple mouse-based computer interaction lie in the range of 60 ms and that inter-individual differences in latency perception can be related to user characteristics.},
  booktitle = {Engineering {{Psychology}} and {{Cognitive Ergonomics}}: {{Cognition}} and {{Design}}},
  series = {Lecture Notes in Computer Science},
  publisher = {{Springer International Publishing}},
  date = {2017},
  pages = {45-56},
  keywords = {Human-computer interaction,Latency,Latency perception,Mouse-based interaction,System response time},
  author = {Forch, Valentin and Franke, Thomas and Rauh, Nadine and Krems, Josef F.},
  editor = {Harris, Don},
  file = {/home/robin/Dropbox/Biblio/Forch_Frankeet-al_2017_Are_100_ms_Fast_Enough.pdf},
  note = {00001}
}

@book{ware_information_2012,
  title = {Information Visualization: Perception for Design},
  shorttitle = {Information Visualization},
  publisher = {{Elsevier}},
  date = {2012},
  author = {Ware, Colin},
  file = {/home/robin/Dropbox/Biblio/Ware_2012_Information_visualization.pdf;/home/robin/.mozilla/firefox/2czgwusc.default/zotero/storage/HHZQRUZG/books.html},
  note = {05270}
}

@book{shneiderman_designing_2004,
  title = {Designing the {{User Interface}}: {{Strategies}} for {{Effective Human}}-{{Computer Interaction}} (4th {{Edition}})},
  isbn = {978-0-321-19786-3},
  shorttitle = {Designing the {{User Interface}}},
  publisher = {{Pearson Addison Wesley}},
  date = {2004},
  keywords = {design,IHM,visual analytics},
  author = {Shneiderman, Ben and Plaisant, Catherine},
  file = {/home/robin/Dropbox/Biblio/Shneiderman_Plaisant_2004_Designing_the_User_Interface.pdf},
  note = {00000}
}

@article{chang_shiny_2015,
  title = {Shiny: Web Application Framework for {{R}}},
  volume = {1},
  shorttitle = {Shiny},
  number = {4},
  journaltitle = {R package version 0.11},
  date = {2015},
  pages = {106},
  author = {Chang, Winston and Cheng, Joe and Allaire, Joseph J. and Xie, Yihui and McPherson, Jonathan},
  note = {00553}
}

@online{plotly_introducing_2017,
  title = {Introducing {{Dash}}},
  url = {https://medium.com/@plotlygraphs/introducing-dash-5ecf7191b503},
  abstract = {Create Reactive Web Apps in pure Python},
  journaltitle = {plotly},
  urldate = {2018-09-12},
  date = {2017-06-21T14:53:12.308Z},
  author = {Plotly},
  file = {/home/robin/.mozilla/firefox/2czgwusc.default/zotero/storage/7KSSIUAR/introducing-dash-5ecf7191b503.html},
  note = {00000}
}

@software{gowda_escher_2018,
  title = {Escher - {{Composable Web UIs}} in {{Julia}}.},
  url = {https://github.com/JuliaGizmos/Escher.jl},
  organization = {{JuliaGizmos}},
  urldate = {2018-09-12},
  date = {2018-09-11T02:10:22Z},
  author = {Gowda, Shashi},
  origdate = {2014-11-20T11:54:05Z},
  note = {00000}
}

@article{bezanson_julia_2014,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1411.1607},
  primaryClass = {cs},
  title = {Julia: {{A Fresh Approach}} to {{Numerical Computing}}},
  url = {http://arxiv.org/abs/1411.1607},
  shorttitle = {Julia},
  abstract = {Bridging cultures that have often been distant, Julia combines expertise from the diverse fields of computer science and computational science to create a new approach to numerical computing. Julia is designed to be easy and fast. Julia questions notions generally held as "laws of nature" by practitioners of numerical computing: 1. High-level dynamic programs have to be slow. 2. One must prototype in one language and then rewrite in another language for speed or deployment, and 3. There are parts of a system for the programmer, and other parts best left untouched as they are built by the experts. We introduce the Julia programming language and its design --- a dance between specialization and abstraction. Specialization allows for custom treatment. Multiple dispatch, a technique from computer science, picks the right algorithm for the right circumstance. Abstraction, what good computation is really about, recognizes what remains the same after differences are stripped away. Abstractions in mathematics are captured as code through another technique from computer science, generic programming. Julia shows that one can have machine performance without sacrificing human convenience.},
  urldate = {2018-09-12},
  date = {2014-11-06},
  keywords = {Computer Science - Mathematical Software},
  author = {Bezanson, Jeff and Edelman, Alan and Karpinski, Stefan and Shah, Viral B.},
  file = {/home/robin/Dropbox/Biblio/Bezanson_Edelmanet-al_2014_Julia.pdf;/home/robin/.mozilla/firefox/2czgwusc.default/zotero/storage/JZRF2J39/1411.html},
  note = {00550}
}

@article{commenges_r_2014,
  title = {R et Espace: {{Traitement}} de l‚Äôinformation G√©ographique},
  shorttitle = {R et Espace},
  journaltitle = {Groupe ElementR, Framabook, Paris},
  date = {2014},
  author = {Commenges, H. and Beauguitte, L. and Buard, E. and Cura, R. and Le N√©chet, F. and Le Texier, M. and Mathian, H. and Rey, S.},
  note = {00003}
}

@unpublished{cura_creer_2015,
  langid = {french},
  venue = {{Lyon}},
  title = {Cr√©er des documents reproductibles et des applications web interactives d‚Äôanalyse de donn√©es avec R : Knitr \& Shiny},
  url = {https://github.com/RCura/CafeMethodo},
  eventtitle = {EVS-ISIG : Caf√© m√©thodo},
  date = {2015-10-06},
  author = {Cura, Robin},
  note = {00000 
http://umr5600.ish-lyon.cnrs.fr/20150610\_EVS-ISIG\_CafeMethodo}
}

@article{wickham_dplyr_2015,
  title = {Dplyr: {{A}} Grammar of Data Manipulation},
  volume = {3},
  shorttitle = {Dplyr},
  journaltitle = {R package version 0.4},
  date = {2015},
  author = {Wickham, Hadley and Francois, Romain and Henry, Lionel and M√ºller, K.},
  note = {00547}
}

@article{wickham_tidyverse_2017,
  title = {Tidyverse: {{Easily}} Install and Load ‚Äôtidyverse‚Äô Packages},
  volume = {1},
  shorttitle = {Tidyverse},
  number = {1},
  journaltitle = {R package version},
  date = {2017},
  author = {Wickham, Hadley},
  note = {00113}
}

@book{wickham_ggplot2_2016,
  title = {Ggplot2: Elegant Graphics for Data Analysis},
  shorttitle = {Ggplot2},
  publisher = {{Springer}},
  date = {2016},
  author = {Wickham, Hadley},
  file = {/home/robin/.mozilla/firefox/2czgwusc.default/zotero/storage/FM4NFEP7/books.html},
  note = {00082}
}

@book{wilkinson_grammar_2006,
  title = {The Grammar of Graphics},
  publisher = {{Springer Science \& Business Media}},
  date = {2006},
  author = {Wilkinson, Leland},
  file = {/home/robin/Dropbox/Biblio/Wilkinson_2006_The_grammar_of_graphics.pdf;/home/robin/.mozilla/firefox/2czgwusc.default/zotero/storage/MNLH55YZ/books.html},
  origdate = {1999},
  note = {01037}
}

@article{grignard_agent-based_2017,
  title = {Agent-{{Based Visualization}}: {{A Real}}-{{Time Visualization Tool Applied Both}} to {{Data}} and {{Simulation Outputs}}},
  shorttitle = {Agent-{{Based Visualization}}},
  date = {2017},
  author = {Grignard, Arnaud and Drogoul, Alexis},
  file = {/home/robin/.mozilla/firefox/2czgwusc.default/zotero/storage/DI5U3FI5/Grignard et Drogoul - 2017 - Agent-Based Visualization A Real-Time Visualizati.pdf;/home/robin/Dropbox/Biblio/Grignard_Drogoul_2017_Agent-Based_Visualization.pdf},
  note = {00002}
}

@report{mortier_human-data_2014,
  langid = {english},
  location = {{Rochester, NY}},
  title = {Human-{{Data Interaction}}: {{The Human Face}} of the {{Data}}-{{Driven Society}}},
  url = {https://papers.ssrn.com/abstract=2508051},
  shorttitle = {Human-{{Data Interaction}}},
  abstract = {The increasing generation and collection of personal data has created a complex ecosystem, often collaborative but sometimes combative, around companies and individuals engaging in the use of these data. We propose that the interactions between these agents warrants a new topic of study: Human-Data Interaction (HDI). In this paper we discuss how HDI sits at the intersection of various disciplines, including computer science, statistics, sociology, psychology and behavioural economics. We expose the challenges that HDI raises, organised into three core themes of legibility, agency and negotiability, and we present the HDI agenda to open up a dialogue amongst interested parties in the personal and big data ecosystems.},
  number = {ID 2508051},
  institution = {{Social Science Research Network}},
  type = {SSRN Scholarly Paper},
  urldate = {2018-09-17},
  date = {2014-10-01},
  keywords = {Economics,HCI,Human Data,Personal Data,Privacy,Psychology,Visualisation},
  author = {Mortier, Richard and Haddadi, Hamed and Henderson, Tristan and McAuley, Derek and Crowcroft, Jon},
  file = {/home/robin/.mozilla/firefox/2czgwusc.default/zotero/storage/2BGKI5M5/papers.html}
}

@inproceedings{elmqvist_embodied_2011,
  title = {Embodied Human-Data Interaction},
  booktitle = {{{ACM CHI}} 2011 {{Workshop}} ‚Äú{{Embodied Interaction}}: {{Theory}} and {{Practice}} in {{HCI}}},
  date = {2011},
  pages = {104--107},
  author = {Elmqvist, Niklas},
  note = {00016}
}

@book{healy_data_2018,
  langid = {english},
  location = {{S.l.}},
  title = {Data {{Visualization}}: {{A Practical Introduction}}},
  isbn = {978-0-691-18161-5},
  url = {http://socviz.co/},
  shorttitle = {Data {{Visualization}}},
  abstract = {An accessible primer on how to create effective graphics from dataThis book provides students and researchers a hands-on introduction to the principles and practice of data visualization. It explains what makes some graphs succeed while others fail, how to make high-quality figures from data using powerful and reproducible methods, and how to think about data visualization in an honest and effective way.Data Visualization builds the reader‚Äôs expertise in ggplot2, a versatile visualization library for the R programming language. Through a series of worked examples, this accessible primer then demonstrates how to create plots piece by piece, beginning with summaries of single variables and moving on to more complex graphics. Topics include plotting continuous and categorical variables; layering information on graphics; producing effective ‚Äúsmall multiple‚Äù plots; grouping, summarizing, and transforming data for plotting; creating maps; working with the output of statistical models; and refining plots to make them more comprehensible.Effective graphics are essential to communicating ideas and a great way to better understand data. This book provides the practical skills students and practitioners need to visualize quantitative data and get the most out of their research findings.Provides hands-on instruction using R and ggplot2Shows how the ‚Äútidyverse‚Äù of data analysis tools makes working with R easier and more consistentIncludes a library of data sets, code, and functions},
  pagetotal = {304},
  publisher = {{Princeton University Press}},
  date = {2018-12-18},
  author = {Healy, Kieran},
  note = {00000}
}

@inproceedings{reynolds_flocks_1987,
  location = {{New York, NY, USA}},
  title = {Flocks, {{Herds}} and {{Schools}}: {{A Distributed Behavioral Model}}},
  isbn = {978-0-89791-227-3},
  url = {http://doi.acm.org/10.1145/37401.37406},
  doi = {10.1145/37401.37406},
  shorttitle = {Flocks, {{Herds}} and {{Schools}}},
  abstract = {The aggregate motion of a flock of birds, a herd of land animals, or a school of fish is a beautiful and familiar part of the natural world. But this type of complex motion is rarely seen in computer animation. This paper explores an approach based on simulation as an alternative to scripting the paths of each bird individually. The simulated flock is an elaboration of a particle systems, with the simulated birds being the particles. The aggregate motion of the simulated flock is created by a distributed behavioral model much like that at work in a natural flock; the birds choose their own course. Each simulated bird is implemented as an independent actor that navigates according to its local perception of the dynamic environment, the laws of simulated physics that rule its motion, and a set of behaviors programmed into it by the "animator." The aggregate motion of the simulated flock is the result of the dense interaction of the relatively simple behaviors of the individual simulated birds.},
  booktitle = {Proceedings of the 14th {{Annual Conference}} on {{Computer Graphics}} and {{Interactive Techniques}}},
  series = {SIGGRAPH '87},
  publisher = {{ACM}},
  urldate = {2018-09-21},
  date = {1987},
  pages = {25--34},
  author = {Reynolds, Craig W.},
  file = {/home/robin/Dropbox/Biblio/Reynolds_1987_Flocks,_Herds_and_Schools.pdf},
  note = {10133}
}

@book{epstein_growing_1996,
  langid = {Anglais},
  location = {{Washington, D.C}},
  title = {Growing Artificial Societies: Social Science from the Bottom Up},
  isbn = {978-0-262-05053-1},
  shorttitle = {Growing Artificial Societies},
  pagetotal = {228},
  publisher = {{MIT Press}},
  date = {1996-11-29},
  author = {Epstein, Joshua M. and Axtell, Robert L.},
  note = {05176}
}

@book{tufte_visual_2001,
  langid = {Anglais},
  location = {{Cheshire, Conn}},
  title = {The Visual Display of Quantitative Information},
  edition = {2nd edition},
  isbn = {978-0-9613921-4-7},
  pagetotal = {190},
  publisher = {{Graphics Press USA}},
  date = {2001-01-31},
  author = {Tufte, Edward R.},
  note = {11580}
}


